program: proteinworkshop/finetune.py
method: grid
name: pre_trained_inverse_folding_scaling
metric: # Does not matter, as we are using sweep to run the experiment.
  goal: minimize
  name: val/loss/total

parameters:
  task:
    values: [inverse_folding]

  dataset:
    values: [cath]

  encoder:
    values: [schnet, gcpnet, gear_net_edge, tfn, mace, egnn]

  optimiser.optimizer.lr:
    values: [0.0001, 0.0003, 0.00001, 0.001]

  features:
    values: [ca_angles, ca_bb]

  scheduler:
    values: [plateau]

  extras.enforce_tags:
    value: False

  trainer.max_epochs:
    value: 250

  # We don't want to freeze the encoder
  finetune.encoder.freeze:
    value: False

  # Just used for checkpoint selection
  +_pre_train_task:
    values: [
        inverse_folding,
      ]

  +_pt_epoch:
    values: ["epoch_0", "epoch_1", "epoch_2", epoch_3", "epoch_4", "epoch_5", "epoch_6", "epoch_7", "epoch_8", "epoch_9"]

  decoder.residue_type.dropout:
    value: ${hparams.hparams.decoder_dropout}

  # Uses checkpoint_root/pretrain_task/encoder/features/last.ckpt
  ckpt_path:
    values:
      [
        # Note: One needed to manually update this path to point to the one's corresponding `last.ckpt` pre-trained checkpoint
        "/home/jamasba/scratch/pworkshop/ckpts/${_pre_train_task}/${hydra:runtime.choices.encoder}/${hydra:runtime.choices.features}/${_pt_epoch}.ckpt",
      ]

  +test:
    value: True

  trainer:
    value: gpu

  logger:
    value: wandb

  name:
    value: "${hydra:runtime.choices.encoder}_${hydra:runtime.choices.features}_${hydra:runtime.choices.aux_task}_epoch_${_pt_epoch}"

command:
  - ${env}
  - HYDRA_FULL_ERROR=1
  - WANDB_START_METHOD=thread
  - python
  - ${program}
  - ${args_no_hyphens}
