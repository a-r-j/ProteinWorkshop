<!doctype html>
<html class="no-js" lang="en" data-content_root="../">
  <head><meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width,initial-scale=1"/>
    <meta name="color-scheme" content="light dark"><meta name="viewport" content="width=device-width, initial-scale=1" />

        <script async src="https://www.googletagmanager.com/gtag/js?id="></script>
        <script>
            window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
            
            gtag('config', '');
            
        </script>
    <meta property="og:title" content="Models" />
<meta property="og:type" content="website" />
<meta property="og:url" content="configs/model.html" />
<meta property="og:site_name" content="Protein Workshop" />
<meta property="og:description" content="Summary of model types To switch between different encoder architectures, simply change the encoder argument in the launch command. For example: Where<ENCODER_NAME> is given by bracketed name in th..." />
<meta property="og:image:width" content="1146" />
<meta property="og:image:height" content="600" />
<meta property="og:image" content="/_images/social_previews/summary_configs_model_1f1b4e84.png" />
<meta property="og:image:alt" content="Summary of model types To switch between different encoder architectures, simply change the encoder argument in the launch command. For example: Where<ENCODE..." />
<meta name="description" content="Summary of model types To switch between different encoder architectures, simply change the encoder argument in the launch command. For example: Where<ENCODER_NAME> is given by bracketed name in th..." />
<meta name="twitter:card" content="summary_large_image" />
<link rel="index" title="Index" href="../genindex.html" /><link rel="search" title="Search" href="../search.html" /><link rel="next" title="Tasks" href="task.html" /><link rel="prev" title="Dataset" href="dataset.html" />

    <!-- Generated with Sphinx 7.2.4 and Furo 2023.08.19 -->
        <title>Models - Protein Workshop 0.2.1</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=a746c00c" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo.css?v=135e06be" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/tabs.css?v=4c969af8" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-codeautolink.css?v=125d5c1c" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo-extensions.css?v=36a5483c" />
    
    


<style>
  body {
    --color-code-background: #f8f8f8;
  --color-code-foreground: black;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
      }
    }
  }
</style></head>
  <body>
    
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-half" viewBox="0 0 24 24">
    <title>Auto light/dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-shadow">
      <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
      <circle cx="12" cy="12" r="9" />
      <path d="M13 12h5" />
      <path d="M13 15h4" />
      <path d="M13 18h1" />
      <path d="M13 9h4" />
      <path d="M13 6h1" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc">
<label class="overlay sidebar-overlay" for="__navigation">
  <div class="visually-hidden">Hide navigation sidebar</div>
</label>
<label class="overlay toc-overlay" for="__toc">
  <div class="visually-hidden">Hide table of contents sidebar</div>
</label>



<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <div class="visually-hidden">Toggle site navigation sidebar</div>
        <i class="icon"><svg><use href="#svg-menu"></use></svg></i>
      </label>
    </div>
    <div class="header-center">
      <a href="../index.html"><div class="brand">Protein Workshop 0.2.1</div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle">
          <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
          <svg class="theme-icon-when-auto"><use href="#svg-sun-half"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon" for="__toc">
        <div class="visually-hidden">Toggle table of contents sidebar</div>
        <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="sidebar-brand" href="../index.html">
  
  <div class="sidebar-logo-container">
    <img class="sidebar-logo" src="../_static/workshop_icon.png" alt="Logo"/>
  </div>
  
  <span class="sidebar-brand-text">Protein Workshop 0.2.1</span>
  
</a><form class="sidebar-search-container" method="get" action="../search.html" role="search">
  <input class="sidebar-search" placeholder="Search" name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
  <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials.html">Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../overview.html">Architectural Overview</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../quickstart.html">Quickstart</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" role="switch" type="checkbox"/><label for="toctree-checkbox-1"><div class="visually-hidden">Toggle navigation of Quickstart</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../quickstart_component/pretrain.html">Pre-Training</a></li>
<li class="toctree-l2"><a class="reference internal" href="../quickstart_component/downstream.html">Downstream</a></li>
<li class="toctree-l2"><a class="reference internal" href="../quickstart_component/download.html">Downloading Datasets</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Default Configs</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="templates.html">Config Templates</a></li>
<li class="toctree-l1"><a class="reference internal" href="dataset.html">Dataset</a></li>
<li class="toctree-l1 current current-page"><a class="current reference internal" href="#">Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="task.html">Tasks</a></li>
<li class="toctree-l1"><a class="reference internal" href="features.html">Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="transforms.html">Transforms</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../framework.html">Framework Components</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" role="switch" type="checkbox"/><label for="toctree-checkbox-2"><div class="visually-hidden">Toggle navigation of Framework Components</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="framework_components/callbacks.html">Callbacks</a></li>
<li class="toctree-l2"><a class="reference internal" href="framework_components/debug.html">Debug</a></li>
<li class="toctree-l2"><a class="reference internal" href="framework_components/env.html">Environment</a></li>
<li class="toctree-l2"><a class="reference internal" href="framework_components/logger.html">Loggers</a></li>
<li class="toctree-l2"><a class="reference internal" href="framework_components/hydra.html">Hydra Configuration</a></li>
<li class="toctree-l2"><a class="reference internal" href="framework_components/extras.html">Extras</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../ml_components.html">ML Components</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" role="switch" type="checkbox"/><label for="toctree-checkbox-3"><div class="visually-hidden">Toggle navigation of ML Components</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="ml_components/trainer.html">Trainer</a></li>
<li class="toctree-l2"><a class="reference internal" href="ml_components/optimiser.html">Optimiser</a></li>
<li class="toctree-l2"><a class="reference internal" href="ml_components/scheduler.html">Schedulers</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../modules/proteinworkshop.train.html">protein_workshop.train</a></li>
<li class="toctree-l1"><a class="reference internal" href="../modules/proteinworkshop.datasets.html">protein_workshop.datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../modules/proteinworkshop.models.html">protein_workshop.models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../modules/proteinworkshop.tasks.html">protein_workshop.tasks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../modules/proteinworkshop.features.html">protein_workshop.features</a></li>
<li class="toctree-l1"><a class="reference internal" href="../modules/proteinworkshop.utils.html">protein_workshop.utils</a></li>
<li class="toctree-l1"><a class="reference internal" href="../modules/proteinworkshop.types.html">protein_workshop.types</a></li>
</ul>

</div>
</div>

      </div>
      
    </div>
  </aside>
  <div class="main">
    <div class="content">
      <div class="article-container">
        <a href="#" class="back-to-top muted-link">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
          </svg>
          <span>Back to top</span>
        </a>
        <div class="content-icon-container">
          
<div class="theme-toggle-container theme-toggle-content">
            <button class="theme-toggle">
              <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
              <svg class="theme-icon-when-auto"><use href="#svg-sun-half"></use></svg>
              <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
              <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
            </button>
          </div>
          <label class="toc-overlay-icon toc-content-icon" for="__toc">
            <div class="visually-hidden">Toggle table of contents sidebar</div>
            <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
          </label>
        </div>
        <article role="main">
          <section id="models">
<h1>Models<a class="headerlink" href="#models" title="Link to this heading">#</a></h1>
<a class="reference internal image-reference" href="../_images/box_models.png"><img alt="Summary of model types" class="align-center" src="../_images/box_models.png" style="width: 400px;" /></a>
<p>To switch between different encoder architectures, simply change the <code class="docutils literal notranslate"><span class="pre">encoder</span></code> argument in the launch command. For example:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>workshop<span class="w"> </span>train<span class="w"> </span><span class="nv">encoder</span><span class="o">=</span>&lt;ENCODER_NAME&gt;<span class="w"> </span><span class="nv">dataset</span><span class="o">=</span>cath<span class="w"> </span><span class="nv">task</span><span class="o">=</span>inverse_folding<span class="w"> </span><span class="nv">trainer</span><span class="o">=</span>cpu
<span class="c1"># or</span>
python<span class="w"> </span>proteinworkshop/train.py<span class="w"> </span><span class="nv">encoder</span><span class="o">=</span>&lt;ENCODER_NAME&gt;<span class="w"> </span><span class="nv">dataset</span><span class="o">=</span>cath<span class="w"> </span><span class="nv">task</span><span class="o">=</span>inverse_folding<span class="w"> </span><span class="nv">trainer</span><span class="o">=</span>cpu<span class="w"> </span><span class="c1"># or trainer=gpu</span>
</pre></div>
</div>
<p>Where <code class="docutils literal notranslate"><span class="pre">&lt;ENCODER_NAME&gt;</span></code> is given by bracketed name in the listing below. For example, the encoder name for SchNet is <code class="docutils literal notranslate"><span class="pre">schnet</span></code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>To change encoder hyperparameters, either</p>
<ol class="arabic simple">
<li><p>Edit the config file directly, or</p></li>
<li><p>Provide commands in the form:</p></li>
</ol>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>workshop<span class="w"> </span>train<span class="w"> </span><span class="nv">encoder</span><span class="o">=</span>&lt;ENCODER_NAME&gt;<span class="w"> </span>encoder.num_layer<span class="o">=</span><span class="m">3</span><span class="w"> </span>encoder.readout<span class="o">=</span>mean<span class="w"> </span><span class="nv">dataset</span><span class="o">=</span>cath<span class="w"> </span><span class="nv">task</span><span class="o">=</span>inverse_folding<span class="w"> </span><span class="nv">trainer</span><span class="o">=</span>cpu
<span class="c1"># or</span>
python<span class="w"> </span>proteinworkshop/train.py<span class="w"> </span><span class="nv">encoder</span><span class="o">=</span>&lt;ENCODER_NAME&gt;<span class="w"> </span>encoder.num_layer<span class="o">=</span><span class="m">3</span><span class="w"> </span>encoder.readout<span class="o">=</span>mean<span class="w"> </span><span class="nv">dataset</span><span class="o">=</span>cath<span class="w"> </span><span class="nv">task</span><span class="o">=</span>inverse_folding<span class="w"> </span><span class="nv">trainer</span><span class="o">=</span>cpu<span class="w"> </span><span class="c1"># or trainer=gpu</span>
</pre></div>
</div>
</div>
<section id="invariant-encoders">
<h2>Invariant Encoders<a class="headerlink" href="#invariant-encoders" title="Link to this heading">#</a></h2>
<div class="table-wrapper docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Name</p></th>
<th class="head"><p>Source</p></th>
<th class="head"><p>Protein Specific</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">GearNet</span></code></p></td>
<td><p><a class="reference external" href="https://arxiv.org/pdf/2203.06125">Zhang et al.</a></p></td>
<td><p>✓</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">ProNet</span></code></p></td>
<td><p><a class="reference external" href="https://arxiv.org/abs/2207.12600">Wang et al.</a></p></td>
<td><p>✓</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">DimeNet++</span></code></p></td>
<td><p><a class="reference external" href="https://arxiv.org/abs/2011.14115">Gasteiger et al.</a></p></td>
<td><p>✗</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">SchNet</span></code></p></td>
<td><p><a class="reference external" href="https://arxiv.org/abs/1706.08566">Schütt et al.</a></p></td>
<td><p>✗</p></td>
</tr>
</tbody>
</table>
</div>
<section id="schnet-schnet">
<h3><a class="reference internal" href="../modules/proteinworkshop.models.html#proteinworkshop.models.graph_encoders.schnet.SchNetModel" title="proteinworkshop.models.graph_encoders.schnet.SchNetModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">SchNet</span></code></a> (<code class="docutils literal notranslate"><span class="pre">schnet</span></code>)<a class="headerlink" href="#schnet-schnet" title="Link to this heading">#</a></h3>
<p>SchNet is one of the most popular and simplest instantiation of E(3) invariant message passing GNNs. SchNet constructs messages through element-wise multiplication of scalar features modulated by a radial filter conditioned on the pairwise distance <span class="math notranslate nohighlight">\(\Vert \vec{\vx}_{ij} \Vert`\)</span> between two neighbours.
Scalar features are update from iteration <span class="math notranslate nohighlight">\(t`\)</span> to <span class="math notranslate nohighlight">\(t+1\)</span> via:</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\begin{align}
    \vs_i^{(t+1)} &amp; \defeq \vs_i^{(t)} + \sum_{j \in \mathcal{N}_i} f_1 \left( \vs_j^{(t)} , \ \Vert \vec{\vx}_{ij} \Vert \right) \label{eq:schnet}
\end{align}\]</div>
</div>
<div class="literal-block-wrapper docutils container" id="id1">
<div class="code-block-caption"><span class="caption-text">config/encoder/schnet.yaml</span><a class="headerlink" href="#id1" title="Link to this code">#</a></div>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">_target_</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">proteinworkshop.models.graph_encoders.schnet.SchNetModel</span>
<span class="nt">hidden_channels</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">512</span><span class="w"> </span><span class="c1"># Number of channels in the hidden layers</span>
<span class="nt">out_dim</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">32</span><span class="w"> </span><span class="c1"># Output dimension of the model</span>
<span class="nt">num_layers</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">6</span><span class="w"> </span><span class="c1"># Number of filters used in convolutional layers</span>
<span class="nt">num_filters</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">128</span><span class="w"> </span><span class="c1"># Number of convolutional layers in the model</span>
<span class="nt">num_gaussians</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">50</span><span class="w"> </span><span class="c1"># Number of Gaussian functions used for radial filters</span>
<span class="nt">cutoff</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">10.0</span><span class="w"> </span><span class="c1"># Cutoff distance for interactions</span>
<span class="nt">max_num_neighbors</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">32</span><span class="w"> </span><span class="c1"># Maximum number of neighboring atoms to consider</span>
<span class="nt">readout</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;add&quot;</span><span class="w"> </span><span class="c1"># Global pooling method to be used</span>
<span class="nt">dipole</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">False</span>
<span class="nt">mean</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="nt">std</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="nt">atomref</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">null</span>
</pre></div>
</div>
</div>
</section>
<section id="dimenet-dimenet-plus-plus">
<h3><a class="reference internal" href="../modules/proteinworkshop.models.html#proteinworkshop.models.graph_encoders.dimenetpp.DimeNetPPModel" title="proteinworkshop.models.graph_encoders.dimenetpp.DimeNetPPModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">DimeNet++</span></code></a> (<code class="docutils literal notranslate"><span class="pre">dimenet_plus_plus</span></code>)<a class="headerlink" href="#dimenet-dimenet-plus-plus" title="Link to this heading">#</a></h3>
<p>DimeNet is an E(3) invariant GNN which uses both distances <span class="math notranslate nohighlight">\(\Vert \vec{\vx}_{ij} \Vert\)</span> and angles <span class="math notranslate nohighlight">\(\vec{\vx}_{ij} \cdot \vec{\vx}_{ik}\)</span> to perform message passing among triplets, as follows:</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\begin{align}
    \vs_i^{(t+1)} &amp; \defeq \sum_{j \in \mathcal{N}_i} f_1 \Big( \vs_i^{(t)} , \ \vs_j^{(t)} , \sum_{k \in \mathcal{N}_i \backslash \{j\}} f_2 \left( \vs_j^{(t)} , \ \vs_k^{(t)} , \ \Vert \vec{\vx}_{ij} \Vert , \ \vec{\vx}_{ij} \cdot \vec{\vx}_{ik} \right) \Big) \label{eq:dimenet}
\end{align}\]</div>
</div>
<div class="literal-block-wrapper docutils container" id="id2">
<div class="code-block-caption"><span class="caption-text">config/encoder/dimenet_plus_plus.yaml</span><a class="headerlink" href="#id2" title="Link to this code">#</a></div>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">_target_</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">proteinworkshop.models.graph_encoders.dimenetpp.DimeNetPPModel</span>
<span class="nt">hidden_channels</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">512</span><span class="w">  </span><span class="c1"># Number of channels in the hidden layers</span>
<span class="nt">out_dim</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">32</span><span class="w">  </span><span class="c1"># Output dimension of the model</span>
<span class="nt">num_layers</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">6</span><span class="w">  </span><span class="c1"># Number of layers in the model</span>
<span class="nt">int_emb_size</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">64</span><span class="w">  </span><span class="c1"># Embedding size for interaction features</span>
<span class="nt">basis_emb_size</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">8</span><span class="w">  </span><span class="c1"># Embedding size for basis functions</span>
<span class="nt">out_emb_channels</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">256</span><span class="w">  </span><span class="c1"># Number of channels in the output embeddings</span>
<span class="nt">num_spherical</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">7</span><span class="w">  </span><span class="c1"># Number of spherical harmonics</span>
<span class="nt">num_radial</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">6</span><span class="w">  </span><span class="c1"># Number of radial basis functions</span>
<span class="nt">cutoff</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">10.0</span><span class="w">  </span><span class="c1"># Cutoff distance for interactions</span>
<span class="nt">max_num_neighbors</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">32</span><span class="w">  </span><span class="c1"># Maximum number of neighboring atoms to consider</span>
<span class="nt">envelope_exponent</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">5</span><span class="w">  </span><span class="c1"># Exponent of the envelope function</span>
<span class="nt">num_before_skip</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span><span class="w">  </span><span class="c1"># Number of layers before the skip connections</span>
<span class="nt">num_after_skip</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">2</span><span class="w">  </span><span class="c1"># Number of layers after the skip connections</span>
<span class="nt">num_output_layers</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">3</span><span class="w">  </span><span class="c1"># Number of output layers</span>
<span class="nt">act</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;swish&quot;</span><span class="w">  </span><span class="c1"># Activation function to use</span>
<span class="nt">readout</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;add&quot;</span><span class="w">  </span><span class="c1"># Global pooling method to be used</span>
</pre></div>
</div>
</div>
</section>
<section id="gearnet-gear-net-gear-net-edge">
<h3><a class="reference internal" href="../modules/proteinworkshop.models.html#proteinworkshop.models.graph_encoders.gear_net.GearNet" title="proteinworkshop.models.graph_encoders.gear_net.GearNet"><code class="xref py py-class docutils literal notranslate"><span class="pre">GearNet</span></code></a> (<code class="docutils literal notranslate"><span class="pre">gear_net</span></code>, <code class="docutils literal notranslate"><span class="pre">gear_net_edge</span></code>)<a class="headerlink" href="#gearnet-gear-net-gear-net-edge" title="Link to this heading">#</a></h3>
<p>GearNet-Edge is an SE(3) invariant architecture leveraging
relational graph convolutional layers and edge message passing. The original
GearNet-Edge formulation presented in Zhang et al. (2023) operates on
multirelational protein structure graphs making use of several edge construction
schemes (<span class="math notranslate nohighlight">\(k\)</span>-NN, euclidean distance and sequence distance based).
Our benchmark contains full capabilities for working with multirelational graphs
but use a single edge type (i.e. <span class="math notranslate nohighlight">\(|\mathcal{R}| = 1\)</span>) in our experiments to
enable more direct architectural comparisons.</p>
<p>The relational graph convolutional layer is defined for relation type <span class="math notranslate nohighlight">\(r\)</span> as:</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\begin{split}\begin{equation}
    \vs_{i}^{(t+1)} \defeq \vs_i^{(t)} + \sigma \left(\mathrm{BN}\left( \sum_{r \in \mathcal{R}} \mathbf{W_r} \sum_{j \in \mathcal{N}_{r}(i)} \vs_j^{(t)}) \right) \right) \\
\end{equation}\end{split}\]</div>
</div>
<p>The edge message passing layer is defined for relation type <span class="math notranslate nohighlight">\(r\)</span> as:</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\begin{equation}
    \vm_{(i,j,r_{1})}^{(t+1)} \defeq \sigma \left( \mathrm{BN} \left( \sum_{r \in {|R|\prime}} \mathbf{W}^{\prime}_r \sum_{(w, k, r_2) \in \mathcal{N}_{r}^{\prime}((i,j,r_{1}))}\vm_{(w,k,r_{2})}^{(t)}\right)\right)
\end{equation}\]</div>
</div>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\begin{equation}
    \vs_{i}^{(t+1)} \defeq \sigma \left( \mathrm{BN} \left( \sum_{r \in {|R|}} \mathbf{W}_r \sum_{j \in \mathcal{N}_{r}(i)}\left(s_{j}^{(t)} + \mathrm{FC}(\vm_{(i,j,r)}^{(t + 1)})\right)\right)\right),
\end{equation}\]</div>
</div>
<p>where <span class="math notranslate nohighlight">\(\mathrm{FC(\cdot)}\)</span> denotes a linear transformation upon the message function.</p>
<div class="literal-block-wrapper docutils container" id="id3">
<div class="code-block-caption"><span class="caption-text">config/encoder/gear_net.yaml</span><a class="headerlink" href="#id3" title="Link to this code">#</a></div>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">_target_</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">proteinworkshop.models.graph_encoders.gear_net.GearNet</span>
<span class="nt">input_dim</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${resolve_feature_config_dim:${features},scalar_node_features,${task},true}</span><span class="w"> </span><span class="c1"># Dimension of the input node features</span>
<span class="nt">num_relation</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${resolve_num_edge_types:${features}}</span><span class="w"> </span><span class="c1"># Number of edge types</span>
<span class="nt">num_layers</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">6</span><span class="w"> </span><span class="c1"># Number of layers in the model</span>
<span class="nt">emb_dim</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">512</span><span class="w"> </span><span class="c1"># Dimension of the node embeddings</span>
<span class="nt">short_cut</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">True</span><span class="w"> </span><span class="c1"># Whether to use short cut connections</span>
<span class="nt">concat_hidden</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">True</span><span class="w"> </span><span class="c1"># Whether to concatenate hidden representations</span>
<span class="nt">batch_norm</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">True</span><span class="w"> </span><span class="c1"># Whether to use batch norm</span>
<span class="nt">num_angle_bin</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">null</span><span class="w"> </span><span class="c1"># Number of angle bins for edge message passing</span>
<span class="nt">activation</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;relu&quot;</span><span class="w"> </span><span class="c1"># Activation function to use</span>
<span class="nt">pool</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">sum</span><span class="w"> </span><span class="c1"># Pooling operation to use</span>
</pre></div>
</div>
</div>
<div class="literal-block-wrapper docutils container" id="id4">
<div class="code-block-caption"><span class="caption-text">config/encoder/gear_net_edge.yaml</span><a class="headerlink" href="#id4" title="Link to this code">#</a></div>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">_target_</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">proteinworkshop.models.graph_encoders.gear_net.GearNet</span>
<span class="nt">input_dim</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${resolve_feature_config_dim:${features},scalar_node_features,${task},true}</span><span class="w"> </span><span class="c1"># Dimension of the input node features</span>
<span class="nt">num_relation</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${resolve_num_edge_types:${features}}</span><span class="w"> </span><span class="c1"># Number of edge types</span>
<span class="nt">num_layers</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">6</span><span class="w"> </span><span class="c1"># Number of layers in the model</span>
<span class="nt">emb_dim</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">512</span><span class="w"> </span><span class="c1"># Dimension of the node embeddings</span>
<span class="nt">short_cut</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">True</span><span class="w"> </span><span class="c1"># Whether to use short cut connections</span>
<span class="nt">concat_hidden</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">True</span><span class="w"> </span><span class="c1"># Whether to concatenate hidden representations</span>
<span class="nt">batch_norm</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">True</span><span class="w"> </span><span class="c1"># Whether to use batch norm</span>
<span class="nt">num_angle_bin</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">7</span><span class="w"> </span><span class="c1"># Number of angle bins for edge message passing</span>
<span class="nt">activation</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;relu&quot;</span><span class="w"> </span><span class="c1"># Activation function to use</span>
<span class="nt">pool</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">sum</span><span class="w"> </span><span class="c1"># Pooling operation to use</span>
</pre></div>
</div>
</div>
</section>
</section>
<section id="vector-equivariant-encoders">
<h2>Vector-Equivariant Encoders<a class="headerlink" href="#vector-equivariant-encoders" title="Link to this heading">#</a></h2>
<div class="table-wrapper docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Name</p></th>
<th class="head"><p>Source</p></th>
<th class="head"><p>Protein Specific</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">GCPNet</span></code></p></td>
<td><p><a class="reference external" href="https://arxiv.org/abs/2211.02504">Morehead et al.</a></p></td>
<td><p>✓</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">GVP-GNN</span></code></p></td>
<td><p><a class="reference external" href="https://arxiv.org/abs/2009.01411">Jing et al.</a></p></td>
<td><p>✓</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">EGNN</span></code></p></td>
<td><p><a class="reference external" href="https://arxiv.org/abs/2102.09844">Satorras et al.</a></p></td>
<td><p>✗</p></td>
</tr>
</tbody>
</table>
</div>
<section id="egnn-egnn">
<h3><a class="reference internal" href="../modules/proteinworkshop.models.html#proteinworkshop.models.graph_encoders.egnn.EGNNModel" title="proteinworkshop.models.graph_encoders.egnn.EGNNModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">EGNN</span></code></a> (<code class="docutils literal notranslate"><span class="pre">egnn</span></code>)<a class="headerlink" href="#egnn-egnn" title="Link to this heading">#</a></h3>
<p>We consider E(3) equivariant GNN layers proposed by Satorras et al. (2021) which updates both scalar features <span class="math notranslate nohighlight">\(\vs_i\)</span> as well as node coordinates <span class="math notranslate nohighlight">\(\vec{\vx}_{i}\)</span>, as follows:</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
    \vs_i^{(t+1)} &amp; \defeq f_2 \left( \vs_i^{(t)} \ , \ \sum_{j \in \mathcal{N}_i} f_1 \left( \vs_i^{(t)} , \vs_j^{(t)} , \ \Vert \vec{\vx}_{ij}^{(t)} \Vert \right) \right)      \\
    \vec{\vx}_i^{(t+1)} &amp; \defeq \vec{\vx}_i^{(t)} + \sum_{j \in \mathcal{N}_i} \vec{\vx}_{ij}^{(t)} \odot f_3 \left( \vs_i^{(t)} , \vs_j^{(t)} , \ \Vert \vec{\vx}_{ij}^{(t)} \Vert \right)
\end{align}\end{split}\]</div>
</div>
<div class="literal-block-wrapper docutils container" id="id5">
<div class="code-block-caption"><span class="caption-text">config/encoder/egnn.yaml</span><a class="headerlink" href="#id5" title="Link to this code">#</a></div>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">_target_</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;proteinworkshop.models.graph_encoders.egnn.EGNNModel&quot;</span>
<span class="nt">num_layers</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">6</span><span class="w"> </span><span class="c1"># Number of message passing layers</span>
<span class="nt">emb_dim</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">512</span><span class="w"> </span><span class="c1"># Dimension of the node embeddings</span>
<span class="nt">activation</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">relu</span><span class="w"> </span><span class="c1"># Activation function to use</span>
<span class="nt">norm</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">layer</span><span class="w"> </span><span class="c1"># Normalisation layer to use</span>
<span class="nt">aggr</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;sum&quot;</span><span class="w"> </span><span class="c1"># Aggregation function to use</span>
<span class="nt">pool</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;sum&quot;</span><span class="w"> </span><span class="c1"># Pooling operation to use</span>
<span class="nt">residual</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">True</span><span class="w"> </span><span class="c1"># Whether to use residual connections</span>
</pre></div>
</div>
</div>
</section>
<section id="gvp-gvp">
<h3><a class="reference internal" href="../modules/proteinworkshop.models.html#proteinworkshop.models.graph_encoders.gvp.GVPGNNModel" title="proteinworkshop.models.graph_encoders.gvp.GVPGNNModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">GVP</span></code></a> (<code class="docutils literal notranslate"><span class="pre">gvp</span></code>)<a class="headerlink" href="#gvp-gvp" title="Link to this heading">#</a></h3>
<div class="literal-block-wrapper docutils container" id="id6">
<div class="code-block-caption"><span class="caption-text">config/encoder/gvp.yaml</span><a class="headerlink" href="#id6" title="Link to this code">#</a></div>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">_target_</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">proteinworkshop.models.graph_encoders.gvp.GVPGNNModel</span>
<span class="nt">s_dim</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">128</span><span class="w">  </span><span class="c1"># Dimension of the node state embeddings</span>
<span class="nt">v_dim</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">16</span><span class="w">  </span><span class="c1"># Dimension of the node vector embeddings</span>
<span class="nt">s_dim_edge</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">32</span><span class="w">  </span><span class="c1"># Dimension of the edge scalar embeddings</span>
<span class="nt">v_dim_edge</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span><span class="w">  </span><span class="c1"># Dimension of the edge vector embeddings</span>
<span class="nt">r_max</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">10.0</span><span class="w">  </span><span class="c1"># Maximum distance for Bessel basis functions</span>
<span class="nt">num_bessel</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">8</span><span class="w">  </span><span class="c1"># Number of Bessel basis functions</span>
<span class="nt">num_polynomial_cutoff</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">8</span><span class="w"> </span><span class="c1">#  Number of polynomial cutoff basis functions</span>
<span class="nt">num_layers</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">5</span><span class="w">  </span><span class="c1"># Number of layers in the model</span>
<span class="nt">pool</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;sum&quot;</span><span class="w">  </span><span class="c1"># Global pooling method to be used</span>
<span class="nt">residual</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">True</span><span class="w">  </span><span class="c1"># Whether to use residual connections</span>
</pre></div>
</div>
</div>
</section>
<section id="gcpnet-gcpnet">
<h3><a class="reference internal" href="../modules/proteinworkshop.models.html#id4" title="proteinworkshop.models.graph_encoders.gcpnet.GCPNetModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">GCPNet</span></code></a> (<code class="docutils literal notranslate"><span class="pre">gcpnet</span></code>)<a class="headerlink" href="#gcpnet-gcpnet" title="Link to this heading">#</a></h3>
<p>GCPNet is an SE(3) equivariant architecture that jointly learns scalar and vector-valued features from geometric protein structure inputs and, through the use of geometry-complete frame embeddings, sensitises its predictions to account for potential changes induced by the effects of molecular chirality on protein structure. In contrast to the original GCPNet formulation presented in Morehead et al. (2022), the implementation we provide in the benchmark incorporates the architectural enhancements proposed in Morehead et al. (2023) which include the addition of a scalar message attention gate (i.e., <span class="math notranslate nohighlight">\(f_{a}(\cdot)`\)</span>) and a simplified structure for the model’s geometric graph convolution layers (i.e., <span class="math notranslate nohighlight">\(f_{n}(\cdot)\)</span>). With geometry-complete graph convolution in mind, for node <span class="math notranslate nohighlight">\(i\)</span> and layer <span class="math notranslate nohighlight">\(t\)</span>, scalar edge features <span class="math notranslate nohighlight">\(\vs_{e^{ij}}^{(t)}\)</span> and vector edge features <span class="math notranslate nohighlight">\(\vv_{e^{ij}}^{(t)}\)</span> are used along with scalar node features <span class="math notranslate nohighlight">\(\vs_{n^{i}}^{(t)}\)</span> and vector node features <span class="math notranslate nohighlight">\(\vv_{n^{i}}^{(t)}\)</span> to update each node feature type as:</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\begin{equation}
    (\vs_{m^{ij}}^{(t+1)}, \vv_{m^{ij}}^{(t+1)}) \defeq f_{e}^{(t+1)}\left((\vs_{n^{i}}^{(t)}, \vv_{n^{i}}^{(t)}),(\vs_{n^{j}}^{(t)}, \vv_{n^{j}}^{(t)}),(f_{a}^{(t + 1)}(\vs_{e^{ij}}^{(t)}), \vv_{e^{ij}}^{(t)}),\mathbf{\mathcal{F}}_{ij}\right)
\end{equation}\]</div>
</div>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\begin{equation}
    (\vs_{n^{i}}^{(t+1)}, \vv_{n^{i}}^{(t+1)}) \defeq f_{n}^{(t+1)}\left((\vs_{n^{i}}^{(t)}, \vv_{n^{i}}^{(t)}), \sum_{j \in \mathcal{N}(i)} (\vs_{m^{ij}}^{(t+1)}, \vv_{m^{ij}}^{(t+1)}) \right),
\end{equation}\]</div>
</div>
<p>where the geometry-complete and chirality-sensitive local frames for node <span class="math notranslate nohighlight">\(i\)</span> (i.e., its edges) are defined as <span class="math notranslate nohighlight">\(\mathbf{\mathcal{F}}_{ij} = (\va_{ij}, \vb_{ij}, \vc_{ij})\)</span>, with <span class="math notranslate nohighlight">\(\va_{ij} = \frac{\vx_{i} - \vx_{j}}{ \lVert \vx_{i} - \vx_{j} \rVert }, \vb_{ij} = \frac{\vx_{i} \times \vx_{j}}{ \lVert \vx_{i} \times \vx_{j} \rVert },\)</span> and <span class="math notranslate nohighlight">\(\vc_{ij} = \va_{ij} \times \vb_{ij}\)</span>, respectively.</p>
<div class="literal-block-wrapper docutils container" id="id7">
<div class="code-block-caption"><span class="caption-text">config/encoder/gcpnet.yaml</span><a class="headerlink" href="#id7" title="Link to this code">#</a></div>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">_target_</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">proteinworkshop.models.graph_encoders.gcpnet.GCPNetModel</span>
<span class="c1"># overrides for feature config #</span>
<span class="nt">features</span><span class="p">:</span>
<span class="w">  </span><span class="c1"># note: will manually be injected into `/features` via `validate_gcpnet_config()` of `proteinworkshop/configs/config.py`</span>
<span class="w">  </span><span class="nt">vector_node_features</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">&quot;orientation&quot;</span><span class="p p-Indicator">]</span>
<span class="w">  </span><span class="nt">vector_edge_features</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">&quot;edge_vectors&quot;</span><span class="p p-Indicator">]</span>
<span class="c1"># global config #</span>
<span class="nt">num_layers</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">6</span><span class="w"> </span><span class="c1"># Number of layers in the model</span>
<span class="nt">emb_dim</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">128</span>
<span class="nt">node_s_emb_dim</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${.emb_dim}</span><span class="w"> </span><span class="c1"># Dimension of the node state embeddings</span>
<span class="nt">node_v_emb_dim</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">16</span><span class="w"> </span><span class="c1"># Dimension of the node vector embeddings</span>
<span class="nt">edge_s_emb_dim</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">32</span><span class="w"> </span><span class="c1"># Dimension of the edge state embeddings</span>
<span class="nt">edge_v_emb_dim</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">4</span><span class="w"> </span><span class="c1"># Dimension of the edge vector embeddings</span>
<span class="nt">r_max</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">10.0</span><span class="w"> </span><span class="c1"># Maximum distance for radial basis functions</span>
<span class="nt">num_rbf</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">8</span><span class="w"> </span><span class="c1"># Number of radial basis functions </span>
<span class="nt">activation</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">silu</span><span class="w"> </span><span class="c1"># Activation function to use in each GCP layer</span>
<span class="nt">pool</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">sum</span><span class="w"> </span><span class="c1"># Global pooling method to be used</span>
<span class="c1"># module config #</span>
<span class="nt">module_cfg</span><span class="p">:</span>
<span class="w">  </span><span class="nt">norm_pos_diff</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="w">  </span><span class="nt">scalar_gate</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0</span>
<span class="w">  </span><span class="nt">vector_gate</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="w">  </span><span class="nt">scalar_nonlinearity</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${..activation}</span>
<span class="w">  </span><span class="nt">vector_nonlinearity</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${..activation}</span>
<span class="w">  </span><span class="nt">nonlinearities</span><span class="p">:</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${..scalar_nonlinearity}</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${..vector_nonlinearity}</span>
<span class="w">  </span><span class="nt">r_max</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${..r_max}</span>
<span class="w">  </span><span class="nt">num_rbf</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${..num_rbf}</span>
<span class="w">  </span><span class="nt">bottleneck</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">4</span>
<span class="w">  </span><span class="nt">vector_linear</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="w">  </span><span class="nt">vector_identity</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="w">  </span><span class="nt">default_bottleneck</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">4</span>
<span class="w">  </span><span class="nt">predict_node_positions</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span><span class="w">  </span><span class="c1"># note: if `false`, then the input node positions will not be updated</span>
<span class="w">  </span><span class="nt">predict_node_rep</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span><span class="w">  </span><span class="c1"># note: if `false`, then a final projection of the node features will not be performed</span>
<span class="w">  </span><span class="nt">node_positions_weight</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1.0</span>
<span class="w">  </span><span class="nt">update_positions_with_vector_sum</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="w">  </span><span class="nt">enable_e3_equivariance</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="w">  </span><span class="nt">pool</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${..pool}</span>
<span class="c1"># model config #</span>
<span class="nt">model_cfg</span><span class="p">:</span>
<span class="w">  </span><span class="nt">h_input_dim</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${resolve_feature_config_dim:${features},scalar_node_features,${task},true}</span>
<span class="w">  </span><span class="nt">chi_input_dim</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${resolve_feature_config_dim:${features},vector_node_features,${task},false}</span>
<span class="w">  </span><span class="nt">e_input_dim</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${plus:${resolve_feature_config_dim:${features},scalar_edge_features,${task},true},${..num_rbf}}</span>
<span class="w">  </span><span class="nt">xi_input_dim</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${resolve_feature_config_dim:${features},vector_edge_features,${task},false}</span>
<span class="w">  </span><span class="c1"># note: each `hidden_dim` must be evenly divisible by `bottleneck`</span>
<span class="w">  </span><span class="nt">h_hidden_dim</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${..node_s_emb_dim}</span>
<span class="w">  </span><span class="nt">chi_hidden_dim</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${..node_v_emb_dim}</span>
<span class="w">  </span><span class="nt">e_hidden_dim</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${..edge_s_emb_dim}</span>
<span class="w">  </span><span class="nt">xi_hidden_dim</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${..edge_v_emb_dim}</span>
<span class="w">  </span><span class="nt">num_layers</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${..num_layers}</span>
<span class="w">  </span><span class="nt">dropout</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.0</span>
<span class="c1"># layer config #</span>
<span class="nt">layer_cfg</span><span class="p">:</span>
<span class="w">  </span><span class="nt">pre_norm</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="w">  </span><span class="nt">use_gcp_norm</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="w">  </span><span class="nt">use_gcp_dropout</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="w">  </span><span class="nt">use_scalar_message_attention</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="w">  </span><span class="nt">num_feedforward_layers</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">2</span>
<span class="w">  </span><span class="nt">dropout</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.0</span>
<span class="w">  </span><span class="nt">nonlinearity_slope</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1e-2</span>
<span class="w">  </span><span class="c1"># message-passing config #</span>
<span class="w">  </span><span class="nt">mp_cfg</span><span class="p">:</span>
<span class="w">    </span><span class="nt">edge_encoder</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="w">    </span><span class="nt">edge_gate</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="w">    </span><span class="nt">num_message_layers</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">4</span>
<span class="w">    </span><span class="nt">message_residual</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0</span>
<span class="w">    </span><span class="nt">message_ff_multiplier</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
<span class="w">    </span><span class="nt">self_message</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
</pre></div>
</div>
</div>
</section>
</section>
<section id="tensor-equivariant-encoders">
<h2>Tensor-Equivariant Encoders<a class="headerlink" href="#tensor-equivariant-encoders" title="Link to this heading">#</a></h2>
<div class="table-wrapper docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Name</p></th>
<th class="head"><p>Source</p></th>
<th class="head"><p>Protein Specific</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">Tensor</span> <span class="pre">Field</span> <span class="pre">Network</span></code></p></td>
<td><p><a class="reference external" href="https://arxiv.org/abs/2210.01776">Corso et al.</a></p></td>
<td><p>❓</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">Multi-ACE</span></code></p></td>
<td><p><a class="reference external" href="https://arxiv.org/abs/2206.07697">Batatia et al.</a></p></td>
<td><p>✗</p></td>
</tr>
</tbody>
</table>
</div>
<section id="tensor-field-networks-tfn">
<h3><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span> <span class="pre">Field</span> <span class="pre">Networks</span></code> (<code class="docutils literal notranslate"><span class="pre">tfn</span></code>)<a class="headerlink" href="#tensor-field-networks-tfn" title="Link to this heading">#</a></h3>
<p>Tensor Field Networks are E(3) or SE(3) equivariant GNNs that have been successfully used in protein structure prediction (Baek et al., 2021) and protein-ligand docking (Corso et al., 2022).
These models use higher order spherical tensors <span class="math notranslate nohighlight">\(\tilde \vh_{i,l} \in \mathbb{R}^{2l+1 \times f}\)</span> as node features, starting from order <span class="math notranslate nohighlight">\(l = 0\)</span> up to arbitrary <span class="math notranslate nohighlight">\(l = L\)</span>.
The first two orders correspond to scalar features <span class="math notranslate nohighlight">\(\vs_i\)</span> and vector features <span class="math notranslate nohighlight">\(\vec{\vv}_i\)</span>, respectively.
The higher order tensors <span class="math notranslate nohighlight">\(\tilde \vh_{i}\)</span> are updated via tensor products <span class="math notranslate nohighlight">\(\otimes\)</span> of neighbourhood features <span class="math notranslate nohighlight">\(\tilde \vh_{j}`\)</span> for all <span class="math notranslate nohighlight">\(j \in \mathcal{N}_i\)</span> with the higher order spherical harmonic representations <span class="math notranslate nohighlight">\(Y\)</span> of the relative displacement <span class="math notranslate nohighlight">\(\frac{\vec{\vx}_{ij}}{\Vert \vec{\vx}_{ij} \Vert} = \hat{\vx}_{ij}\)</span>:</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\begin{align}
    \tilde \vh_{i}^{(t+1)} &amp; \defeq \tilde \vh_{i}^{(t)} + \sum_{j \in \mathcal{N}_i} Y \left( \hat{\vx}_{ij} \right) \otimes_{\vw} \tilde \vh_{j}^{(t)} ,
\end{align}\]</div>
</div>
<p>where the weights <span class="math notranslate nohighlight">\(\vw\)</span> of the tensor product are computed via a learnt radial basis function of the relative distance, i.e. <span class="math notranslate nohighlight">\(\vw = f \left( \Vert \vec{\vx}_{ij} \Vert \right)\)</span>.</p>
<div class="literal-block-wrapper docutils container" id="id8">
<div class="code-block-caption"><span class="caption-text">config/encoder/tfn.yaml</span><a class="headerlink" href="#id8" title="Link to this code">#</a></div>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">_target_</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">proteinworkshop.models.graph_encoders.tfn.TensorProductModel</span>
<span class="nt">r_max</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">10.0</span><span class="w"> </span><span class="c1"># Maximum distance for Bessel basis functions</span>
<span class="nt">num_bessel</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">8</span><span class="w"> </span><span class="c1"># Number of Bessel basis functions</span>
<span class="nt">num_polynomial_cutoff</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">5</span><span class="w"> </span><span class="c1"># Number of polynomial cutoff basis functions</span>
<span class="nt">max_ell</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">2</span><span class="w"> </span><span class="c1"># Maximum degree/order of spherical harmonics basis functions and node feature tensors</span>
<span class="nt">num_layers</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">4</span><span class="w"> </span><span class="c1"># Number of layers in the model</span>
<span class="nt">emb_dim</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">64</span><span class="w"> </span><span class="c1"># Number of hidden channels/embedding dimension for each node feature tensor order</span>
<span class="nt">mlp_dim</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">256</span><span class="w"> </span><span class="c1"># Dimension of MLP for computing tensor product weights</span>
<span class="nt">aggr</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;sum&quot;</span><span class="w"> </span><span class="c1"># Aggregation function to use</span>
<span class="nt">pool</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;sum&quot;</span><span class="w"> </span><span class="c1"># Pooling operation to use</span>
<span class="nt">residual</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">True</span><span class="w"> </span><span class="c1"># Whether to use residual connections</span>
<span class="nt">batch_norm</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">True</span><span class="w"> </span><span class="c1"># Whether to use e3nn batch normalization</span>
<span class="nt">gate</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">False</span><span class="w"> </span><span class="c1"># Whether to use gated non-linearity</span>
</pre></div>
</div>
</div>
</section>
<section id="multi-atomic-cluster-expansion-mace">
<h3><code class="xref py py-class docutils literal notranslate"><span class="pre">Multi-Atomic</span> <span class="pre">Cluster</span> <span class="pre">Expansion</span></code> (<code class="docutils literal notranslate"><span class="pre">mace</span></code>)<a class="headerlink" href="#multi-atomic-cluster-expansion-mace" title="Link to this heading">#</a></h3>
<p>MACE (Batatia et al., 2022) is a higher order E(3) or SE(3) equivariant GNN originally developed for molecular dynamics simulations.
MACE provides an efficient approach to computing high body order equivariant features in the Tensor Field Network framework via Atomic Cluster Expansion:
They first aggregate neighbourhood features analogous to the node update equation for TFN above (the <span class="math notranslate nohighlight">\(A\)</span> functions in Batatia et al. (2022) (eq.9)) and then take <span class="math notranslate nohighlight">\(k-1\)</span> repeated self-tensor products of these neighbourhood features.
In our formalism, this corresponds to:</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\begin{align}
    \label{eq:e3nn-3}
    \tilde \vh_{i}^{(t+1)} &amp; \defeq \underbrace {\tilde \vh_{i}^{(t+1)} \otimes_{\vw} \dots \otimes_{\vw} \tilde \vh_{i}^{(t+1)} }_\text{$k-1$ times} \ ,
\end{align}\]</div>
</div>
<div class="literal-block-wrapper docutils container" id="id9">
<div class="code-block-caption"><span class="caption-text">config/encoder/mace.yaml</span><a class="headerlink" href="#id9" title="Link to this code">#</a></div>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">_target_</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">proteinworkshop.models.graph_encoders.mace.MACEModel</span>
<span class="nt">r_max</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">10.0</span><span class="w"> </span><span class="c1"># Maximum distance for Bessel basis functions</span>
<span class="nt">num_bessel</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">8</span><span class="w"> </span><span class="c1"># Number of Bessel basis functions</span>
<span class="nt">num_polynomial_cutoff</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">5</span><span class="w"> </span><span class="c1"># Number of polynomial cutoff basis functions</span>
<span class="nt">max_ell</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">2</span><span class="w"> </span><span class="c1"># Maximum degree/order of spherical harmonics basis functions and node feature tensors</span>
<span class="nt">correlation</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">3</span><span class="w"> </span><span class="c1"># Correlation order (= body order - 1) for Equivariant Product Basis operation</span>
<span class="nt">num_layers</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">4</span><span class="w"> </span><span class="c1"># Number of layers in the model</span>
<span class="nt">emb_dim</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">64</span><span class="w"> </span><span class="c1"># Number of hidden channels/embedding dimension for each node feature tensor order</span>
<span class="nt">mlp_dim</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">256</span><span class="w"> </span><span class="c1"># Dimension of MLP for computing tensor product weights</span>
<span class="nt">aggr</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;sum&quot;</span><span class="w"> </span><span class="c1"># Aggregation function to use</span>
<span class="nt">pool</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;sum&quot;</span><span class="w"> </span><span class="c1"># Pooling operation to use</span>
<span class="nt">residual</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">True</span><span class="w"> </span><span class="c1"># Whether to use residual connections</span>
<span class="nt">batch_norm</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">True</span><span class="w"> </span><span class="c1"># Whether to use e3nn batch normalization</span>
</pre></div>
</div>
</div>
</section>
</section>
<section id="decoder-models">
<h2>Decoder Models<a class="headerlink" href="#decoder-models" title="Link to this heading">#</a></h2>
<p>Decoder models are used to predict the target property from the learned representation.</p>
<p>Decoder configs are dictionaries indexed by the name of the output to which they are applied.</p>
<p>These are configured in the task config. See <a class="reference internal" href="task.html"><span class="doc">Tasks</span></a> for more details.</p>
<p>For example, the <code class="docutils literal notranslate"><span class="pre">residue_type</span></code> decoder:</p>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="reference internal" href="task.html"><span class="doc">Tasks</span></a></p>
</div>
<div class="literal-block-wrapper docutils container" id="id10">
<div class="code-block-caption"><span class="caption-text">config/decoder/residue_type.yaml</span><a class="headerlink" href="#id10" title="Link to this code">#</a></div>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">residue_type</span><span class="p">:</span>
<span class="w">  </span><span class="nt">_target_</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;proteinworkshop.models.decoders.mlp_decoder.MLPDecoder&quot;</span>
<span class="w">  </span><span class="nt">hidden_dim</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">128</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="nv">128</span><span class="p p-Indicator">]</span>
<span class="w">  </span><span class="nt">dropout</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.0</span><span class="w"> </span><span class="c1"># dropout rate</span>
<span class="w">  </span><span class="nt">activations</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">&quot;relu&quot;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">&quot;relu&quot;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">&quot;none&quot;</span><span class="p p-Indicator">]</span>
<span class="w">  </span><span class="nt">skip</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;concat&quot;</span><span class="w"> </span><span class="c1"># Or sum/False</span>
<span class="w">  </span><span class="nt">out_dim</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">23</span>
<span class="w">  </span><span class="nt">input</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;node_embedding&quot;</span>
</pre></div>
</div>
</div>
</section>
</section>

        </article>
      </div>
      <footer>
        
        <div class="related-pages">
          <a class="next-page" href="task.html">
              <div class="page-info">
                <div class="context">
                  <span>Next</span>
                </div>
                <div class="title">Tasks</div>
              </div>
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
            </a>
          <a class="prev-page" href="dataset.html">
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
              <div class="page-info">
                <div class="context">
                  <span>Previous</span>
                </div>
                
                <div class="title">Dataset</div>
                
              </div>
            </a>
        </div>
        <div class="bottom-of-page">
          <div class="left-details">
            <div class="copyright">
                Copyright &#169; 2023, Arian R. Jamasb
            </div>
            Made with <a href="https://www.sphinx-doc.org/">Sphinx</a> and <a class="muted-link" href="https://pradyunsg.me">@pradyunsg</a>'s
            
            <a href="https://github.com/pradyunsg/furo">Furo</a>
            
          </div>
          <div class="right-details">
            
          </div>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer">
      
      
      <div class="toc-sticky toc-scroll">
        <div class="toc-title-container">
          <span class="toc-title">
            On this page
          </span>
        </div>
        <div class="toc-tree-container">
          <div class="toc-tree">
            <ul>
<li><a class="reference internal" href="#">Models</a><ul>
<li><a class="reference internal" href="#invariant-encoders">Invariant Encoders</a><ul>
<li><a class="reference internal" href="#schnet-schnet"><code class="xref py py-class docutils literal notranslate"><span class="pre">SchNet</span></code> (<code class="docutils literal notranslate"><span class="pre">schnet</span></code>)</a></li>
<li><a class="reference internal" href="#dimenet-dimenet-plus-plus"><code class="xref py py-class docutils literal notranslate"><span class="pre">DimeNet++</span></code> (<code class="docutils literal notranslate"><span class="pre">dimenet_plus_plus</span></code>)</a></li>
<li><a class="reference internal" href="#gearnet-gear-net-gear-net-edge"><code class="xref py py-class docutils literal notranslate"><span class="pre">GearNet</span></code> (<code class="docutils literal notranslate"><span class="pre">gear_net</span></code>, <code class="docutils literal notranslate"><span class="pre">gear_net_edge</span></code>)</a></li>
</ul>
</li>
<li><a class="reference internal" href="#vector-equivariant-encoders">Vector-Equivariant Encoders</a><ul>
<li><a class="reference internal" href="#egnn-egnn"><code class="xref py py-class docutils literal notranslate"><span class="pre">EGNN</span></code> (<code class="docutils literal notranslate"><span class="pre">egnn</span></code>)</a></li>
<li><a class="reference internal" href="#gvp-gvp"><code class="xref py py-class docutils literal notranslate"><span class="pre">GVP</span></code> (<code class="docutils literal notranslate"><span class="pre">gvp</span></code>)</a></li>
<li><a class="reference internal" href="#gcpnet-gcpnet"><code class="xref py py-class docutils literal notranslate"><span class="pre">GCPNet</span></code> (<code class="docutils literal notranslate"><span class="pre">gcpnet</span></code>)</a></li>
</ul>
</li>
<li><a class="reference internal" href="#tensor-equivariant-encoders">Tensor-Equivariant Encoders</a><ul>
<li><a class="reference internal" href="#tensor-field-networks-tfn"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span> <span class="pre">Field</span> <span class="pre">Networks</span></code> (<code class="docutils literal notranslate"><span class="pre">tfn</span></code>)</a></li>
<li><a class="reference internal" href="#multi-atomic-cluster-expansion-mace"><code class="xref py py-class docutils literal notranslate"><span class="pre">Multi-Atomic</span> <span class="pre">Cluster</span> <span class="pre">Expansion</span></code> (<code class="docutils literal notranslate"><span class="pre">mace</span></code>)</a></li>
</ul>
</li>
<li><a class="reference internal" href="#decoder-models">Decoder Models</a></li>
</ul>
</li>
</ul>

          </div>
        </div>
      </div>
      
      
    </aside>
  </div>
</div><script src="../_static/jquery.js?v=5d32c60e"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
    <script src="../_static/documentation_options.js?v=37f418d5"></script>
    <script src="../_static/doctools.js?v=888ff710"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/scripts/furo.js?v=32e29ea5"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/tabs.js?v=3ee01567"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.1.10/require.min.js"></script>
    <script src="http://ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}, "TeX": {"Macros": {"defeq": ":=", "vx": "\\mathbf{x}", "va": "\\mathbf{a}", "vb": "\\mathbf{b}", "vc": "\\mathbf{c}", "vd": "\\mathbf{d}", "ve": "\\mathbf{e}", "vf": "\\mathbf{f}", "vg": "\\mathbf{g}", "vh": "\\mathbf{h}", "vi": "\\mathbf{i}", "vj": "\\mathbf{j}", "vk": "\\mathbf{k}", "vl": "\\mathbf{l}", "vm": "\\mathbf{m}", "vn": "\\mathbf{n}", "vo": "\\mathbf{o}", "vp": "\\mathbf{p}", "vq": "\\mathbf{q}", "vr": "\\mathbf{r}", "vs": "\\mathbf{s}", "vt": "\\mathbf{t}", "vu": "\\mathbf{u}", "vv": "\\mathbf{v}", "vw": "\\mathbf{w}", "vy": "\\mathbf{y}", "vz": "\\mathbf{z}"}}})</script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    </body>
</html>