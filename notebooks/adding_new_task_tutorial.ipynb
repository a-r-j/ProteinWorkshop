{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `ProteinWorkshop` Tutorial, Part 5 - Adding a New Task\n",
    "![Tasks](../docs/source/_static/box_downstream_tasks.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# %load_ext blackcellmagic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the last entry in the `ProteinWorkshop` tutorial series!\n",
    "\n",
    "The task is the centerpiece of our framework. It is the thing you will use to specify other things like the dataset or the model you want to use. It is also the thing that will be used to specify the loss function and the metrics you want to use. In this tutorial, we will show you how to add a new task to the `ProteinWorkshop`.\n",
    "\n",
    "To add your custom task to the `ProteinWorkshop`, you just have to follow the following 4-step procedure (created files in brackets):\n",
    "\n",
    "1. Create a new subclass of the `BaseTransform` class from `torch_geometric.transforms`(`my_new_task.py`)\n",
    "2. Create a new task config file to accompany the custom `BaseTransform` (`my_new_task.yaml`)\n",
    "3. Compose and instantiate your config for pre-training or finetuning using your task\n",
    "4. Use your custom task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create a new subclass of the `BaseTransform` class from `torch_geometric.transforms`(`my_new_task.py`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference the `SequenceNoiseTransform` below (i.e., `proteinworkshop/tasks/sequence_denoising.py`) to fill out a custom `proteinworkshop/tasks/my_new_task.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "class SequenceNoiseTransform(BaseTransform):\n",
    "    def __init__(\n",
    "        self, corruption_rate: float, corruption_strategy: Literal[\"mutate\", \"mask\"]\n",
    "    ):\n",
    "        self.corruption_rate = corruption_rate\n",
    "        self.corruption_strategy = corruption_strategy\n",
    "\n",
    "    @property\n",
    "    def required_attributes(self) -> Set[str]:\n",
    "        return {\"residue_type\"}\n",
    "\n",
    "    @jaxtyped(typechecker=typechecker)\n",
    "    def __call__(self, x: Union[Data, Protein]) -> Union[Data, Protein]:\n",
    "        x.residue_type_uncorrupted = copy.deepcopy(x.residue_type)\n",
    "        # Get indices of residues to corrupt\n",
    "        indices = torch.randint(\n",
    "            0,\n",
    "            x.residue_type.shape[0],\n",
    "            (int(x.residue_type.shape[0] * self.corruption_rate),),\n",
    "            device=x.residue_type.device,\n",
    "        ).long()\n",
    "\n",
    "        # Apply corruption\n",
    "        if self.corruption_strategy == \"mutate\":\n",
    "            # Set indices to random residue type\n",
    "            x.residue_type[indices] = torch.randint(\n",
    "                0,\n",
    "                23,  # TODO: probably best to not hardcode this\n",
    "                (indices.shape[0],),\n",
    "                device=x.residue_type.device,\n",
    "            )\n",
    "        elif self.corruption_strategy == \"mask\":\n",
    "            # Set indices to 23 -> \"UNK\"\n",
    "            x.residue_type[indices] = 23  # TODO: probably best to not hardcode this\n",
    "        else:\n",
    "            raise NotImplementedError(\n",
    "                f\"Corruption strategy: {self.corruption_strategy} not supported.\"\n",
    "            )\n",
    "        # Get indices of applied corruptions\n",
    "        index = torch.zeros(x.residue_type.shape[0])\n",
    "        index[indices] = 1\n",
    "        x.sequence_corruption_mask = index.bool()\n",
    "\n",
    "        return x\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{self.__class__}(corruption_strategy: {self.corruption_strategy} corruption_rate: {self.corruption_rate})\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create a new task config file to accompany the custom `BaseTransform` (`my_new_task.yaml`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference the `sequence_denoising` config below (i.e., `proteinworkshop/config/task/sequence_denoising.yaml`) to fill out a custom `proteinworkshop/config/task/my_new_task.yaml`.\n",
    "\n",
    "This config file sets the actual values of the parameters of your task. This includes default options like the metrics, the decoder or the transforms to use for your dataset, as well as options for your specific dataset, the callbacks used and other things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# @package _global_\n",
    "\n",
    "defaults:\n",
    "  - override /metrics:\n",
    "      - accuracy\n",
    "      - f1_score\n",
    "      - perplexity\n",
    "  - override /decoder:\n",
    "      - residue_type\n",
    "  - override /transforms:\n",
    "      - remove_missing_ca\n",
    "      - sequence_denoising\n",
    "\n",
    "dataset:\n",
    "  num_classes: 23\n",
    "\n",
    "callbacks:\n",
    "  early_stopping:\n",
    "    monitor: val/residue_type/accuracy\n",
    "    mode: \"max\"\n",
    "  model_checkpoint:\n",
    "    monitor: val/residue_type/accuracy\n",
    "    mode: \"max\"\n",
    "\n",
    "task:\n",
    "  task: \"sequence_denoising\"\n",
    "  classification_type: \"multiclass\"\n",
    "  metric_average: \"micro\"\n",
    "\n",
    "  losses:\n",
    "    residue_type: cross_entropy\n",
    "  label_smoothing: 0.0\n",
    "\n",
    "  output:\n",
    "    - residue_type\n",
    "  supervise_on:\n",
    "    - residue_type\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Compose and instantiate your config for pre-training or finetuning using your task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to use the created config file in our code. To do this, we use `Hydra`, a library that helps with managing configuration options via `.yaml` files.\n",
    "\n",
    "In the following code block, we initialize Hydra and then compose the `cfg` object which we will later use to perform downstream or pre-training tasks. We can pass `hydra.compose` various overrides in order to customize our setup. We can specify for example:\n",
    "- the encoder to use\n",
    "- the task to perform later on (here our task `my_new_task`)\n",
    "- the dataset to use\n",
    "- the features that are used\n",
    "- which auxiliary test should be performed (if any)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Misc. tools\n",
    "import os\n",
    "\n",
    "# Hydra tools\n",
    "import hydra\n",
    "\n",
    "from hydra.compose import GlobalHydra\n",
    "from hydra.core.hydra_config import HydraConfig\n",
    "\n",
    "from proteinworkshop.constants import HYDRA_CONFIG_PATH\n",
    "from proteinworkshop.utils.notebook import init_hydra_singleton\n",
    "\n",
    "version_base = \"1.2\"  # Note: Need to update whenever Hydra is upgraded\n",
    "init_hydra_singleton(reload=True, version_base=version_base)\n",
    "\n",
    "path = HYDRA_CONFIG_PATH\n",
    "rel_path = os.path.relpath(path, start=\".\")\n",
    "\n",
    "GlobalHydra.instance().clear()\n",
    "hydra.initialize(rel_path, version_base=version_base)\n",
    "\n",
    "cfg = hydra.compose(\n",
    "    config_name=\"train\",\n",
    "    overrides=[\n",
    "        \"encoder=schnet\",\n",
    "        \"task=my_new_task\",\n",
    "        \"dataset=afdb_swissprot_v4\",\n",
    "        \"features=ca_angles\",\n",
    "        \"+aux_task=none\",\n",
    "    ],\n",
    "    return_hydra_config=True,\n",
    ")\n",
    "\n",
    "# Note: Customize as needed e.g., when running a sweep\n",
    "cfg.hydra.job.num = 0\n",
    "cfg.hydra.job.id = 0\n",
    "cfg.hydra.hydra_help.hydra_help = False\n",
    "cfg.hydra.runtime.output_dir = \"outputs\"\n",
    "\n",
    "HydraConfig.instance().set_config(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Use your custom task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with the config object created, you can make use of the infrastructure that `ProteinWorkshop` provides in order to directly use the config object for training or finetuning a model, depending on what your goal is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proteinworkshop.configs import config\n",
    "from proteinworkshop.finetune import finetune\n",
    "from proteinworkshop.train import train_model\n",
    "\n",
    "cfg = config.validate_config(cfg)\n",
    "\n",
    "# train_model(cfg)  # Pre-train a model using the selected data\n",
    "# finetune(cfg)  # Fine-tune a model using the selected data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we instantiated the config, we specified `ca_angles` as feature context. However, we can easily reconfigure the custom dataset to use side-chain atom context as you can see in the following code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version_base = \"1.2\"  # Note: Need to update whenever Hydra is upgraded\n",
    "init_hydra_singleton(reload=True, version_base=version_base)\n",
    "\n",
    "path = HYDRA_CONFIG_PATH\n",
    "rel_path = os.path.relpath(path, start=\".\")\n",
    "\n",
    "GlobalHydra.instance().clear()\n",
    "hydra.initialize(rel_path, version_base=version_base)\n",
    "\n",
    "cfg = hydra.compose(\n",
    "    config_name=\"train\",\n",
    "    overrides=[\n",
    "        \"encoder=schnet\",\n",
    "        \"task=my_new_task\",\n",
    "        \"dataset=afdb_swissprot_v4\",\n",
    "        \"features=ca_sc\",\n",
    "        \"+aux_task=none\",\n",
    "    ],\n",
    "    return_hydra_config=True,\n",
    ")\n",
    "\n",
    "# Note: Customize as needed e.g., when running a sweep\n",
    "cfg.hydra.job.num = 0\n",
    "cfg.hydra.job.id = 0\n",
    "cfg.hydra.hydra_help.hydra_help = False\n",
    "cfg.hydra.runtime.output_dir = \"outputs\"\n",
    "\n",
    "HydraConfig.instance().set_config(cfg)\n",
    "\n",
    "cfg = config.validate_config(cfg)\n",
    "\n",
    "# train_model(cfg)  # Pre-train a model using the selected data\n",
    "# finetune(cfg)  # Fine-tune a model using the selected data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Wrapping up\n",
    "\n",
    "Have any additional questions about adding your custom task to the `ProteinWorkshop`? [Create a new issue](https://github.com/a-r-j/ProteinWorkshop/issues/new/choose) on our [GitHub repository](https://github.com/a-r-j/ProteinWorkshop). We would be happy to work with you to add your new task to the repository!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
