{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Protein Workshop` Tutorial, Part 5 - Adding a New Task\n",
    "![Tasks](../docs/source/_static/box_downstream_tasks.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add a custom task to the `Protein Workshop`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#%load_ext blackcellmagic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a new subclass of the `torch_geometric.transforms.BaseTransform` class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reference the `SequenceNoiseTransform` below (i.e., `src/tasks/sequence_denoising.py`) to fill out a custom `src/tasks/my_new_task.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "class SequenceNoiseTransform(BaseTransform):\n",
    "    def __init__(\n",
    "        self, corruption_rate: float, corruption_strategy: Literal[\"mutate\", \"mask\"]\n",
    "    ):\n",
    "        self.corruption_rate = corruption_rate\n",
    "        self.corruption_strategy = corruption_strategy\n",
    "\n",
    "    @property\n",
    "    def required_attributes(self) -> Set[str]:\n",
    "        return {\"residue_type\"}\n",
    "\n",
    "    @beartype\n",
    "    def __call__(self, x: Union[Data, Protein]) -> Union[Data, Protein]:\n",
    "        x.residue_type_uncorrupted = copy.deepcopy(x.residue_type)\n",
    "        # Get indices of residues to corrupt\n",
    "        indices = torch.randint(\n",
    "            0,\n",
    "            x.residue_type.shape[0],\n",
    "            (int(x.residue_type.shape[0] * self.corruption_rate),),\n",
    "            device=x.residue_type.device,\n",
    "        ).long()\n",
    "\n",
    "        # Apply corruption\n",
    "        if self.corruption_strategy == \"mutate\":\n",
    "            # Set indices to random residue type\n",
    "            x.residue_type[indices] = torch.randint(\n",
    "                0,\n",
    "                23,  # TODO: probably best to not hardcode this\n",
    "                (indices.shape[0],),\n",
    "                device=x.residue_type.device,\n",
    "            )\n",
    "        elif self.corruption_strategy == \"mask\":\n",
    "            # Set indices to 23 -> \"UNK\"\n",
    "            x.residue_type[indices] = 23  # TODO: probably best to not hardcode this\n",
    "        else:\n",
    "            raise NotImplementedError(\n",
    "                f\"Corruption strategy: {self.corruption_strategy} not supported.\"\n",
    "            )\n",
    "        # Get indices of applied corruptions\n",
    "        index = torch.zeros(x.residue_type.shape[0])\n",
    "        index[indices] = 1\n",
    "        x.sequence_corruption_mask = index.bool()\n",
    "\n",
    "        return x\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{self.__class__}(corruption_strategy: {self.corruption_strategy} corruption_rate: {self.corruption_rate})\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a new task config file to accompany the custom `MyNewTask`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reference the `sequence_denoising` config below (i.e., `configs/task/sequence_denoising.yaml`) to fill out a custom `configs/task/my_new_task.yaml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# @package _global_\n",
    "\n",
    "defaults:\n",
    "  - override /metrics:\n",
    "      - accuracy\n",
    "      - f1_score\n",
    "      - perplexity\n",
    "  - override /decoder:\n",
    "      - residue_type\n",
    "  - override /transforms:\n",
    "      - remove_missing_ca\n",
    "      - sequence_denoising\n",
    "\n",
    "dataset:\n",
    "  num_classes: 23\n",
    "\n",
    "callbacks:\n",
    "  early_stopping:\n",
    "    monitor: val/residue_type/accuracy\n",
    "    mode: \"max\"\n",
    "  model_checkpoint:\n",
    "    monitor: val/residue_type/accuracy\n",
    "    mode: \"max\"\n",
    "\n",
    "task:\n",
    "  task: \"sequence_denoising\"\n",
    "  classification_type: \"multiclass\"\n",
    "  metric_average: \"micro\"\n",
    "\n",
    "  losses:\n",
    "    residue_type: cross_entropy\n",
    "  label_smoothing: 0.0\n",
    "\n",
    "  output:\n",
    "    - residue_type\n",
    "  supervise_on:\n",
    "    - residue_type\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use new task in either a pre-training or fine-tuning regime, including or excluding full-atom context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Misc. tools\n",
    "import os\n",
    "\n",
    "# Hydra tools\n",
    "import hydra\n",
    "\n",
    "from hydra.compose import GlobalHydra\n",
    "from hydra.core.hydra_config import HydraConfig\n",
    "\n",
    "from proteinworkshop.constants import HYDRA_CONFIG_PATH\n",
    "from proteinworkshop.utils.notebook import init_hydra_singleton\n",
    "\n",
    "version_base = \"1.2\"  # Note: Need to update whenever Hydra is upgraded\n",
    "init_hydra_singleton(reload=True, version_base=version_base)\n",
    "\n",
    "path = HYDRA_CONFIG_PATH\n",
    "rel_path = os.path.relpath(path, start=\".\")\n",
    "\n",
    "GlobalHydra.instance().clear()\n",
    "hydra.initialize(rel_path, version_base=version_base)\n",
    "\n",
    "cfg = hydra.compose(config_name=\"train\", overrides=[\"encoder=schnet\", \"task=my_new_task\", \"dataset=afdb_swissprot_v4\", \"features=ca_angles\", \"+aux_task=none\"], return_hydra_config=True)\n",
    "\n",
    "# Note: Customize as needed e.g., when running a sweep\n",
    "cfg.hydra.job.num = 0\n",
    "cfg.hydra.job.id = 0\n",
    "cfg.hydra.hydra_help.hydra_help = False\n",
    "cfg.hydra.runtime.output_dir = \"outputs\"\n",
    "\n",
    "HydraConfig.instance().set_config(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Either pre-train or fine-tune a model using the new task and an existing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proteinworkshop.configs import config\n",
    "from proteinworkshop.finetune import finetune\n",
    "from proteinworkshop.train import train_model\n",
    "\n",
    "cfg = config.validate_config(cfg)\n",
    "\n",
    "# train_model(cfg)  # Pre-train a model using the selected data\n",
    "# finetune(cfg)  # Fine-tune a model using the selected data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconfigure the custom task to incorporate side-chain atom context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version_base = \"1.2\"  # Note: Need to update whenever Hydra is upgraded\n",
    "init_hydra_singleton(reload=True, version_base=version_base)\n",
    "\n",
    "path = HYDRA_CONFIG_PATH\n",
    "rel_path = os.path.relpath(path, start=\".\")\n",
    "\n",
    "GlobalHydra.instance().clear()\n",
    "hydra.initialize(rel_path, version_base=version_base)\n",
    "\n",
    "cfg = hydra.compose(config_name=\"train\", overrides=[\"encoder=schnet\", \"task=my_new_task\", \"dataset=afdb_swissprot_v4\", \"features=ca_sc\", \"+aux_task=none\"], return_hydra_config=True)\n",
    "\n",
    "# Note: Customize as needed e.g., when running a sweep\n",
    "cfg.hydra.job.num = 0\n",
    "cfg.hydra.job.id = 0\n",
    "cfg.hydra.hydra_help.hydra_help = False\n",
    "cfg.hydra.runtime.output_dir = \"outputs\"\n",
    "\n",
    "HydraConfig.instance().set_config(cfg)\n",
    "\n",
    "cfg = config.validate_config(cfg)\n",
    "\n",
    "# train_model(cfg)  # Pre-train a model using the selected data\n",
    "# finetune(cfg)  # Fine-tune a model using the selected data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
