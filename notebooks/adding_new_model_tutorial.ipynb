{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Protein Workshop Tutorial, Part 4 - Adding a New Model\n",
    "![Models](../docs/source/_static/box_models.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add a custom model to the Protein Workshop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#%load_ext blackcellmagic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a new subclass of the `nn.Module` class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reference the `EGNNModel` below (i.e., `proteinworkshop/models/graph_encoders/egnn.py`) to fill out a custom `proteinworkshop/models/graph_encoders/my_new_model.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "class EGNNModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_layers: int = 5,\n",
    "        emb_dim: int = 128,\n",
    "        activation: str = \"relu\",\n",
    "        norm: str = \"layer\",\n",
    "        aggr: str = \"sum\",\n",
    "        pool: str = \"sum\",\n",
    "        residual: bool = True\n",
    "    ):\n",
    "        '''E(n) Equivariant GNN model\n",
    "\n",
    "        Args:\n",
    "            num_layers: (int) - number of message passing layers\n",
    "            emb_dim: (int) - hidden dimension\n",
    "            in_dim: (int) - initial node feature dimension\n",
    "            out_dim: (int) - output number of classes\n",
    "            activation: (str) - non-linearity within MLPs (swish/relu)\n",
    "            norm: (str) - normalisation layer (layer/batch)\n",
    "            aggr: (str) - aggregation function `\\oplus` (sum/mean/max)\n",
    "            pool: (str) - global pooling function (sum/mean)\n",
    "            residual: (bool) - whether to use residual connections\n",
    "        '''\n",
    "        super().__init__()\n",
    "\n",
    "        # Embedding lookup for initial node features\n",
    "        self.emb_in = torch.nn.LazyLinear(emb_dim)\n",
    "\n",
    "        # Stack of GNN layers\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            self.convs.append(EGNNLayer(emb_dim, activation, norm, aggr))\n",
    "\n",
    "        # Global pooling/readout function\n",
    "        self.pool = get_aggregation(pool)\n",
    "\n",
    "        self.residual = residual\n",
    "\n",
    "    @property\n",
    "    def required_batch_attributes(self) -> Set[str]:\n",
    "        return {\"x\", \"pos\", \"edge_index\", \"batch\"}\n",
    "\n",
    "    def forward(self, batch) -> EncoderOutput:\n",
    "        h = self.emb_in(batch.x)  # (n,) -> (n, d)\n",
    "        pos = batch.pos  # (n, 3)\n",
    "\n",
    "        for conv in self.convs:\n",
    "            # Message passing layer\n",
    "            h_update, pos_update = conv(h, pos, batch.edge_index)\n",
    "\n",
    "            # Update node features (n, d) -> (n, d)\n",
    "            h = h + h_update if self.residual else h_update\n",
    "\n",
    "            # Update node coordinates (no residual) (n, 3) -> (n, 3)\n",
    "            pos = pos_update\n",
    "\n",
    "        return EncoderOutput({\n",
    "            \"node_embedding\": h,\n",
    "            \"graph_embedding\": self.pool(h, batch.batch),  # (n, d) -> (batch_size, d)\n",
    "            \"pos\": pos # Position\n",
    "        })\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a new model config file to accompany the custom `MyNewModel`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reference the `EGNN` config below (i.e., `configs/encoder/egnn.yaml`) to fill out a custom `configs/encoder/my_new_model.yaml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "_target_: \"proteinworkshop.models.graph_encoders.egnn.EGNNModel\"\n",
    "num_layers: 6\n",
    "emb_dim: 512\n",
    "activation: relu\n",
    "norm: layer\n",
    "aggr: \"sum\"\n",
    "pool: \"sum\"\n",
    "residual: True\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use new model in either a pre-training or fine-tuning regime, with or without full-atom context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Misc. tools\n",
    "import os\n",
    "\n",
    "# Hydra tools\n",
    "import hydra\n",
    "\n",
    "from hydra.compose import GlobalHydra\n",
    "from hydra.core.hydra_config import HydraConfig\n",
    "\n",
    "from proteinworkshop.constants import HYDRA_CONFIG_PATH\n",
    "from proteinworkshop.utils.notebook import init_hydra_singleton\n",
    "\n",
    "version_base = \"1.2\"  # Note: Need to update whenever Hydra is upgraded\n",
    "init_hydra_singleton(reload=True, version_base=version_base)\n",
    "\n",
    "path = HYDRA_CONFIG_PATH\n",
    "rel_path = os.path.relpath(path, start=\".\")\n",
    "\n",
    "GlobalHydra.instance().clear()\n",
    "hydra.initialize(rel_path, version_base=version_base)\n",
    "\n",
    "cfg = hydra.compose(config_name=\"train\", overrides=[\"encoder=my_new_model\", \"task=inverse_folding\", \"dataset=afdb_swissprot_v4\", \"features=ca_angles\", \"+aux_task=none\"], return_hydra_config=True)\n",
    "\n",
    "# Note: Customize as needed e.g., when running a sweep\n",
    "cfg.hydra.job.num = 0\n",
    "cfg.hydra.job.id = 0\n",
    "cfg.hydra.hydra_help.hydra_help = False\n",
    "cfg.hydra.runtime.output_dir = \"outputs\"\n",
    "\n",
    "HydraConfig.instance().set_config(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Either pre-train or fine-tune the custom model using an existing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proteinworkshop.configs import config\n",
    "from proteinworkshop.finetune import finetune\n",
    "from proteinworkshop.train import train_model\n",
    "\n",
    "cfg = config.validate_config(cfg)\n",
    "\n",
    "# train_model(cfg)  # Pre-train a model using the selected data\n",
    "# finetune(cfg)  # Fine-tune a model using the selected data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconfigure the custom model to leverage side-chain atom context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version_base = \"1.2\"  # Note: Need to update whenever Hydra is upgraded\n",
    "init_hydra_singleton(reload=True, version_base=version_base)\n",
    "\n",
    "path = HYDRA_CONFIG_PATH\n",
    "rel_path = os.path.relpath(path, start=\".\")\n",
    "\n",
    "GlobalHydra.instance().clear()\n",
    "hydra.initialize(rel_path, version_base=version_base)\n",
    "\n",
    "cfg = hydra.compose(config_name=\"train\", overrides=[\"encoder=my_new_model\", \"task=inverse_folding\", \"dataset=afdb_swissprot_v4\", \"features=ca_sc\", \"+aux_task=none\"], return_hydra_config=True)\n",
    "\n",
    "# Note: Customize as needed e.g., when running a sweep\n",
    "cfg.hydra.job.num = 0\n",
    "cfg.hydra.job.id = 0\n",
    "cfg.hydra.hydra_help.hydra_help = False\n",
    "cfg.hydra.runtime.output_dir = \"outputs\"\n",
    "\n",
    "HydraConfig.instance().set_config(cfg)\n",
    "\n",
    "cfg = config.validate_config(cfg)\n",
    "\n",
    "# train_model(cfg)  # Pre-train a model using the selected data\n",
    "# finetune(cfg)  # Fine-tune a model using the selected data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
