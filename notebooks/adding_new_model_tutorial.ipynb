{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `ProteinWorkshop` Tutorial, Part 4 - Adding a New Model\n",
    "![Models](../docs/source/_static/box_models.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#%load_ext blackcellmagic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ProteinWorkshop` encomposses several models as well as pre-trained weights for them so that you can readily use them. However, you may want to add your own model to the `ProteinWorkshop` to fulfill a specific use case. This tutorial will show you how to do that.\n",
    "\n",
    "To add your custom model to the `ProteinWorkshop`, you just have to follow the following 4-step procedure (created files in brackets):\n",
    "\n",
    "1. Create a new subclass of the `nn.Module` class (`my_new_model.py`).\n",
    "2. Create a new model config file to accompany the custom `MyNewModel` (`my_new_model.yaml`).\n",
    "3. Compose and instantiate your config for pre-training or finetuning using your model\n",
    "4. Use your custom model in a pre-training or finetuning task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a new subclass of the `nn.Module` class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference the `EGNNModel` below (i.e., `proteinworkshop/models/graph_encoders/egnn.py`) to fill out a custom `proteinworkshop/models/graph_encoders/my_new_model.py` in a similar style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "class EGNNModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_layers: int = 5,\n",
    "        emb_dim: int = 128,\n",
    "        activation: str = \"relu\",\n",
    "        norm: str = \"layer\",\n",
    "        aggr: str = \"sum\",\n",
    "        pool: str = \"sum\",\n",
    "        residual: bool = True\n",
    "    ):\n",
    "        '''E(n) Equivariant GNN model\n",
    "\n",
    "        Args:\n",
    "            num_layers: (int) - number of message passing layers\n",
    "            emb_dim: (int) - hidden dimension\n",
    "            in_dim: (int) - initial node feature dimension\n",
    "            out_dim: (int) - output number of classes\n",
    "            activation: (str) - non-linearity within MLPs (swish/relu)\n",
    "            norm: (str) - normalisation layer (layer/batch)\n",
    "            aggr: (str) - aggregation function `\\oplus` (sum/mean/max)\n",
    "            pool: (str) - global pooling function (sum/mean)\n",
    "            residual: (bool) - whether to use residual connections\n",
    "        '''\n",
    "        super().__init__()\n",
    "\n",
    "        # Embedding lookup for initial node features\n",
    "        self.emb_in = torch.nn.LazyLinear(emb_dim)\n",
    "\n",
    "        # Stack of GNN layers\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            self.convs.append(EGNNLayer(emb_dim, activation, norm, aggr))\n",
    "\n",
    "        # Global pooling/readout function\n",
    "        self.pool = get_aggregation(pool)\n",
    "\n",
    "        self.residual = residual\n",
    "\n",
    "    @property\n",
    "    def required_batch_attributes(self) -> Set[str]:\n",
    "        return {\"x\", \"pos\", \"edge_index\", \"batch\"}\n",
    "\n",
    "    def forward(self, batch) -> EncoderOutput:\n",
    "        h = self.emb_in(batch.x)  # (n,) -> (n, d)\n",
    "        pos = batch.pos  # (n, 3)\n",
    "\n",
    "        for conv in self.convs:\n",
    "            # Message passing layer\n",
    "            h_update, pos_update = conv(h, pos, batch.edge_index)\n",
    "\n",
    "            # Update node features (n, d) -> (n, d)\n",
    "            h = h + h_update if self.residual else h_update\n",
    "\n",
    "            # Update node coordinates (no residual) (n, 3) -> (n, 3)\n",
    "            pos = pos_update\n",
    "\n",
    "        return EncoderOutput({\n",
    "            \"node_embedding\": h,\n",
    "            \"graph_embedding\": self.pool(h, batch.batch),  # (n, d) -> (batch_size, d)\n",
    "            \"pos\": pos # Position\n",
    "        })\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create a new model config file to accompany the custom `MyNewModel`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference the `EGNN` config below (i.e., `configs/encoder/egnn.yaml`) to fill out a custom `configs/encoder/my_new_model.yaml`. This config file sets the actual values of the parameters of your model. The parameters present here will depend on the model you implemented; in the case of the `EGNN` model shown as demonstration in this tutorial, these parameters include the number of layers, the embedding dimension and the activation function used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "_target_: \"proteinworkshop.models.graph_encoders.egnn.EGNNModel\"\n",
    "num_layers: 6\n",
    "emb_dim: 512\n",
    "activation: relu\n",
    "norm: layer\n",
    "aggr: \"sum\"\n",
    "pool: \"sum\"\n",
    "residual: True\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Compose and instantiate your config for pre-training or finetuning using your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to use the created config file in our code. To do this, we use `Hydra`, a library that helps with managing configuration options via `.yaml` files.\n",
    "\n",
    "In the following code block, we initialize Hydra and then compose the `cfg` object which we will later use to perform downstream or pre-training tasks. We can pass `hydra.compose` various overrides in order to customize our setup. We can specify for example:\n",
    "- the encoder to use (here our new custom model)\n",
    "- the task to perform later on\n",
    "- the dataset to use\n",
    "- the features that are used\n",
    "- which auxiliary test should be performed (if any)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Misc. tools\n",
    "import os\n",
    "\n",
    "# Hydra tools\n",
    "import hydra\n",
    "\n",
    "from hydra.compose import GlobalHydra\n",
    "from hydra.core.hydra_config import HydraConfig\n",
    "\n",
    "from proteinworkshop.constants import HYDRA_CONFIG_PATH\n",
    "from proteinworkshop.utils.notebook import init_hydra_singleton\n",
    "\n",
    "version_base = \"1.2\"  # Note: Need to update whenever Hydra is upgraded\n",
    "init_hydra_singleton(reload=True, version_base=version_base)\n",
    "\n",
    "path = HYDRA_CONFIG_PATH\n",
    "rel_path = os.path.relpath(path, start=\".\")\n",
    "\n",
    "GlobalHydra.instance().clear()\n",
    "hydra.initialize(rel_path, version_base=version_base)\n",
    "\n",
    "cfg = hydra.compose(config_name=\"train\", overrides=[\"encoder=my_new_model\", \"task=inverse_folding\", \"dataset=afdb_swissprot_v4\", \"features=ca_angles\", \"+aux_task=none\"], return_hydra_config=True)\n",
    "\n",
    "# Note: Customize as needed e.g., when running a sweep\n",
    "cfg.hydra.job.num = 0\n",
    "cfg.hydra.job.id = 0\n",
    "cfg.hydra.hydra_help.hydra_help = False\n",
    "cfg.hydra.runtime.output_dir = \"outputs\"\n",
    "\n",
    "HydraConfig.instance().set_config(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Use your custom model in a pre-training or finetuning task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with the config object created, you can make use of the infrastructure that the Protein Workshop provides in order to directly use the config object for training or finetuning a model, depending on what your goal is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proteinworkshop.configs import config\n",
    "from proteinworkshop.finetune import finetune\n",
    "from proteinworkshop.train import train_model\n",
    "\n",
    "cfg = config.validate_config(cfg)\n",
    "\n",
    "# train_model(cfg)  # Pre-train a model using the selected data\n",
    "# finetune(cfg)  # Fine-tune a model using the selected data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we instantiated the config, we specified `ca_angles` as feature context. However, we can easily reconfigure the custom model to use side-chain atom context as you can see in the following code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version_base = \"1.2\"  # Note: Need to update whenever Hydra is upgraded\n",
    "init_hydra_singleton(reload=True, version_base=version_base)\n",
    "\n",
    "path = HYDRA_CONFIG_PATH\n",
    "rel_path = os.path.relpath(path, start=\".\")\n",
    "\n",
    "GlobalHydra.instance().clear()\n",
    "hydra.initialize(rel_path, version_base=version_base)\n",
    "\n",
    "cfg = hydra.compose(config_name=\"train\", overrides=[\"encoder=my_new_model\", \"task=inverse_folding\", \"dataset=afdb_swissprot_v4\", \"features=ca_sc\", \"+aux_task=none\"], return_hydra_config=True)\n",
    "\n",
    "# Note: Customize as needed e.g., when running a sweep\n",
    "cfg.hydra.job.num = 0\n",
    "cfg.hydra.job.id = 0\n",
    "cfg.hydra.hydra_help.hydra_help = False\n",
    "cfg.hydra.runtime.output_dir = \"outputs\"\n",
    "\n",
    "HydraConfig.instance().set_config(cfg)\n",
    "\n",
    "cfg = config.validate_config(cfg)\n",
    "\n",
    "# train_model(cfg)  # Pre-train a model using the selected data\n",
    "# finetune(cfg)  # Fine-tune a model using the selected data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Wrapping up\n",
    "\n",
    "Have any additional questions about adding your custom model to the `ProteinWorkshop`? [Create a new issue](https://github.com/a-r-j/ProteinWorkshop/issues/new/choose) on our [GitHub repository](https://github.com/a-r-j/ProteinWorkshop). We would be happy to work with you to add your new model to the repository!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
