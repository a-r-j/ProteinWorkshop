{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `ProteinWorkshop` Tutorial, Part 3 - Adding a New Dataset\n",
    "![Datasets](../docs/source/_static/box_datasets.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#%load_ext blackcellmagic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ProteinWorkshop` encomposses several datasets for pre-training and finetuning. However, you may want to add your own dataset to the `ProteinWorkshop` to fulfill a specific use case. This tutorial will show you how to do that.\n",
    "\n",
    "To add your custom dataset to the `ProteinWorkshop`, you just have to follow the following 4-step procedure (created files in brackets):\n",
    "\n",
    "1. Create a new subclass of the `ProteinDataModule` class (`my_new_dataset.py`)\n",
    "2. Create a new data config file to accompany the custom `MyNewDataModule` (`my_new_dataset.yaml`)\n",
    "3. Compose and instantiate your config for pre-training or finetuning using your dataset\n",
    "4. Use your custom dataset for a pre-training or finetuning task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create a new subclass of the `ProteinDataModule` class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference the `CATHDataModule` below (i.e., `proteinworkshop/datasets/cath.py`) to fill out a custom `proteinworkshop/datasets/my_new_dataset.py` in a similar style. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "class CATHDataModule(ProteinDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        path: str,\n",
    "        batch_size: int,\n",
    "        format: str = \"mmtf\",\n",
    "        pdb_dir: Optional[str] = None,\n",
    "        pin_memory: bool = True,\n",
    "        in_memory: bool = False,\n",
    "        num_workers: int = 16,\n",
    "        dataset_fraction: float = 1.0,\n",
    "        transforms: Optional[Iterable[Callable]] = None,\n",
    "        ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.data_dir = Path(path)\n",
    "        self.raw_dir = self.data_dir / \"raw\"\n",
    "        self.processed_dir = self.data_dir / \"processed\"\n",
    "        if not os.path.exists(self.data_dir):\n",
    "            os.makedirs(self.data_dir)\n",
    "\n",
    "        if transforms is not None:\n",
    "            self.transform = self.compose_transforms(\n",
    "                omegaconf.OmegaConf.to_container(\n",
    "                    transforms,\n",
    "                    resolve=True\n",
    "                    )\n",
    "                )\n",
    "        else:\n",
    "            self.transform = None\n",
    "\n",
    "        self.in_memory = in_memory\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.pin_memory = pin_memory\n",
    "        self.num_workers = num_workers\n",
    "        self.format = format\n",
    "        self.pdb_dir = pdb_dir\n",
    "\n",
    "        self.dataset_fraction = dataset_fraction\n",
    "        self.excluded_chains: List[str] = self.exclude_pdbs()\n",
    "\n",
    "    def download(self):\n",
    "        self.download_chain_list()\n",
    "\n",
    "    def parse_labels(self):\n",
    "        pass\n",
    "\n",
    "    def exclude_pdbs(self):\n",
    "        return []\n",
    "\n",
    "    def download_chain_list(self):  # sourcery skip: move-assign\n",
    "        URL = \"http://people.csail.mit.edu/ingraham/graph-protein-design/data/cath/chain_set_splits.json\"\n",
    "        if not os.path.exists(self.data_dir / \"chain_set_splits.json\"):\n",
    "            logger.info(\"Downloading dataset index file...\")\n",
    "            wget.download(URL, str(self.data_dir / \"chain_set_splits.json\"))\n",
    "        else:\n",
    "            logger.info(\"Found existing dataset index\")\n",
    "\n",
    "    @functools.lru_cache\n",
    "    def parse_dataset(self) -> Dict[str, List[str]]:\n",
    "        fpath = self.data_dir / \"chain_set_splits.json\"\n",
    "\n",
    "        with open(fpath, \"r\") as file:\n",
    "            data = json.load(file)\n",
    "\n",
    "        self.train_pdbs = data[\"train\"]\n",
    "        logger.info(f\"Found {len(self.train_pdbs)} chains in training set\")\n",
    "        logger.info(\"Removing obsolete PDBs from training set\")\n",
    "        self.train_pdbs = [pdb for pdb in self.train_pdbs if pdb[:4] not in self.obsolete_pdbs.keys()]\n",
    "        logger.info(f\"{len(self.train_pdbs)} remaining training chains\")\n",
    "\n",
    "        logger.info(f\"Sampling fraction {self.dataset_fraction} of training set\")\n",
    "        fraction = int(self.dataset_fraction * len(self.train_pdbs))\n",
    "        self.train_pdbs = random.sample(self.train_pdbs, fraction)\n",
    "\n",
    "        self.val_pdbs = data[\"validation\"]\n",
    "        logger.info(f\"Found {len(self.val_pdbs)} chains in validation set\")\n",
    "        logger.info(\"Removing obsolete PDBs from validation set\")\n",
    "        self.val_pdbs = [pdb for pdb in self.val_pdbs if pdb[:4] not in self.obsolete_pdbs.keys()]\n",
    "        logger.info(f\"{len(self.val_pdbs)} remaining validation chains\")\n",
    "\n",
    "        self.test_pdbs = data[\"test\"]\n",
    "        logger.info(f\"Found {len(self.test_pdbs)} chains in test set\")\n",
    "        logger.info(\"Removing obsolete PDBs from test set\")\n",
    "        self.test_pdbs = [pdb for pdb in self.test_pdbs if pdb[:4] not in self.obsolete_pdbs.keys()]\n",
    "        logger.info(f\"{len(self.test_pdbs)} remaining test chains\")\n",
    "        return data\n",
    "\n",
    "    def train_dataset(self):\n",
    "        if not hasattr(self, \"train_pdbs\"):\n",
    "            self.parse_dataset()\n",
    "        pdb_codes = [pdb.split(\".\")[0] for pdb in self.train_pdbs]\n",
    "        chains = [pdb.split(\".\")[1] for pdb in self.train_pdbs]\n",
    "\n",
    "        return ProteinDataset(\n",
    "            root=str(self.data_dir),\n",
    "            pdb_dir=self.pdb_dir,\n",
    "            pdb_codes=pdb_codes,\n",
    "            chains=chains,\n",
    "            transform=self.transform,\n",
    "            format=self.format,\n",
    "            in_memory=self.in_memory\n",
    "        )\n",
    "\n",
    "    def val_dataset(self) -> ProteinDataset:\n",
    "        if not hasattr(self, \"val_pdbs\"):\n",
    "            self.parse_dataset()\n",
    "\n",
    "        pdb_codes = [pdb.split(\".\")[0] for pdb in self.val_pdbs]\n",
    "        chains = [pdb.split(\".\")[1] for pdb in self.val_pdbs]\n",
    "\n",
    "        return ProteinDataset(\n",
    "            root=str(self.data_dir),\n",
    "            pdb_dir=self.pdb_dir,\n",
    "            pdb_codes=pdb_codes,\n",
    "            chains=chains,\n",
    "            transform=self.transform,\n",
    "            format=self.format,\n",
    "            in_memory=self.in_memory\n",
    "        )\n",
    "\n",
    "    def test_dataset(self) -> ProteinDataset:\n",
    "        if not hasattr(self, \"test_pdbs\"):\n",
    "            self.parse_dataset()\n",
    "        pdb_codes = [pdb.split(\".\")[0] for pdb in self.test_pdbs]\n",
    "        chains = [pdb.split(\".\")[1] for pdb in self.test_pdbs]\n",
    "\n",
    "        return ProteinDataset(\n",
    "            root=str(self.data_dir),\n",
    "            pdb_dir=self.pdb_dir,\n",
    "            pdb_codes=pdb_codes,\n",
    "            chains=chains,\n",
    "            transform=self.transform,\n",
    "            format=self.format,\n",
    "            in_memory=self.in_memory\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self) -> ProteinDataLoader:\n",
    "        if not hasattr(self, \"train_ds\"):\n",
    "            self.train_ds = self.train_dataset()\n",
    "        return ProteinDataLoader(\n",
    "            self.train_ds,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            drop_last=True,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=self.pin_memory,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self) -> ProteinDataLoader:\n",
    "        if not hasattr(self, \"val_ds\"):\n",
    "            self.val_ds = self.val_dataset()\n",
    "        return ProteinDataLoader(\n",
    "            self.val_ds,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            drop_last=True,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=self.pin_memory,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self) -> ProteinDataLoader:\n",
    "        if not hasattr(self, \"test_ds\"):\n",
    "            self.test_ds = self.test_dataset()\n",
    "        return ProteinDataLoader(\n",
    "            self.test_ds,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            drop_last=True,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=self.pin_memory,\n",
    "        )\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create a new data config file to accompany the custom `MyNewDataModule`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference the `CATH` config below (i.e., `configs/dataset/cath.yaml`) to fill out a custom `configs/dataset/my_new_dataset.yaml`. This config file sets the actual values of the parameters of your datamodule. This includes dataset options like the path where the dataset is stored, but also datamodule options for creating dataloaders like the number of workers or the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "datamodule:\n",
    "  _target_: \"proteinworkshop.datasets.cath.CATHDataModule\"\n",
    "  path: ${env.paths.data}/cath/ # Directory where the dataset is stored\n",
    "  pdb_dir: ${env.paths.data}/pdb/ # Directory where raw PDB/mmtf files are stored\n",
    "  format: \"mmtf\" # Format of the raw PDB/MMTF files\n",
    "  num_workers: 4 # Number of workers for dataloader\n",
    "  pin_memory: True # Pin memory for dataloader\n",
    "  batch_size: 32 # Batch size for dataloader\n",
    "  dataset_fraction: 1.0 # Fraction of the dataset to use\n",
    "  transforms: ${transforms} # Transforms to apply to dataset examples\n",
    "  num_classes: 23 # Number of classes\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Compose and instantiate your config for pre-training or finetuning using your dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to use the created config file in our code. To do this, we use `Hydra`, a library that helps with managing configuration options via `.yaml` files.\n",
    "\n",
    "In the following code block, we initialize Hydra and then compose the `cfg` object which we will later use to perform downstream or pre-training tasks. We can pass `hydra.compose` various overrides in order to customize our setup. We can specify for example:\n",
    "- the encoder to use\n",
    "- the task to perform later on\n",
    "- the dataset to use (here our new custom dataset)\n",
    "- the features that are used\n",
    "- which auxiliary test should be performed (if any)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Misc. tools\n",
    "import os\n",
    "\n",
    "# Hydra tools\n",
    "import hydra\n",
    "\n",
    "from hydra.compose import GlobalHydra\n",
    "from hydra.core.hydra_config import HydraConfig\n",
    "\n",
    "from proteinworkshop.constants import HYDRA_CONFIG_PATH\n",
    "from proteinworkshop.utils.notebook import init_hydra_singleton\n",
    "\n",
    "version_base = \"1.2\"  # Note: Need to update whenever Hydra is upgraded\n",
    "init_hydra_singleton(reload=True, version_base=version_base)\n",
    "\n",
    "path = HYDRA_CONFIG_PATH\n",
    "rel_path = os.path.relpath(path, start=\".\")\n",
    "\n",
    "GlobalHydra.instance().clear()\n",
    "hydra.initialize(rel_path, version_base=version_base)\n",
    "\n",
    "cfg = hydra.compose(config_name=\"train\", overrides=[\"encoder=schnet\", \"task=inverse_folding\", \"dataset=my_new_dataset\", \"features=ca_angles\", \"+aux_task=none\"], return_hydra_config=True)\n",
    "\n",
    "# Note: Customize as needed e.g., when running a sweep\n",
    "cfg.hydra.job.num = 0\n",
    "cfg.hydra.job.id = 0\n",
    "cfg.hydra.hydra_help.hydra_help = False\n",
    "cfg.hydra.runtime.output_dir = \"outputs\"\n",
    "\n",
    "HydraConfig.instance().set_config(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After configuring your config object via `hydra.compose`, you can instantiate your datamodule via `hydra.utils.instantiate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proteinworkshop.configs import config\n",
    "\n",
    "cfg = config.validate_config(cfg)\n",
    "\n",
    "datamodule = hydra.utils.instantiate(cfg.dataset.datamodule)\n",
    "datamodule.setup(\"train\")\n",
    "dl = datamodule.train_dataloader()\n",
    "\n",
    "for i in dl:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Use your custom dataset for a pre-training or finetuning task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of instantiating your datamodule to use it directly, you can make use of the infrastructure that `ProteinWorkshop` provides in order to directly use the config object for training or finetuning a model, depending on what your goal is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proteinworkshop.finetune import finetune\n",
    "from proteinworkshop.train import train_model\n",
    "\n",
    "# train_model(cfg)  # Pre-train a model using the selected data\n",
    "# finetune(cfg)  # Fine-tune a model using the selected data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we instantiated the config, we specified `ca_angles` as feature context. However, we can easily reconfigure the custom dataset to use side-chain atom context as you can see in the following code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version_base = \"1.2\"  # Note: Need to update whenever Hydra is upgraded\n",
    "init_hydra_singleton(reload=True, version_base=version_base)\n",
    "\n",
    "path = HYDRA_CONFIG_PATH\n",
    "rel_path = os.path.relpath(path, start=\".\")\n",
    "\n",
    "GlobalHydra.instance().clear()\n",
    "hydra.initialize(rel_path, version_base=version_base)\n",
    "\n",
    "cfg = hydra.compose(config_name=\"train\", overrides=[\"encoder=schnet\", \"task=inverse_folding\", \"dataset=my_new_dataset\", \"features=ca_sc\", \"+aux_task=none\"], return_hydra_config=True)\n",
    "\n",
    "# Note: Customize as needed e.g., when running a sweep\n",
    "cfg.hydra.job.num = 0\n",
    "cfg.hydra.job.id = 0\n",
    "cfg.hydra.hydra_help.hydra_help = False\n",
    "cfg.hydra.runtime.output_dir = \"outputs\"\n",
    "\n",
    "HydraConfig.instance().set_config(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can verify that side-chain torsions are available as feature inputs. This procedure is not only valid for the features, but can be used to reconfigure the encoder, the task and all the other options specified in the config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proteinworkshop.configs import config\n",
    "\n",
    "cfg = config.validate_config(cfg)\n",
    "\n",
    "datamodule = hydra.utils.instantiate(cfg)\n",
    "datamodule.setup(\"train\")\n",
    "dl = datamodule.train_dataloader()\n",
    "\n",
    "for i in dl:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Wrapping up\n",
    "\n",
    "Have any additional questions about adding your custom dataset to the `ProteinWorkshop`? [Create a new issue](https://github.com/a-r-j/ProteinWorkshop/issues/new/choose) on our [GitHub repository](https://github.com/a-r-j/ProteinWorkshop). We would be happy to work with you to add your new dataset to the repository!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
