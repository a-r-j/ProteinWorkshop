\section{Results \& Discussions}

\subsection{Auxiliary Sequence Denoising Consistently Improves Baseline Performance}

In Table \ref{tab:baseline_graph_classification_results}, we first set out to determine the following questions about architectural choices in conjunction with denoising auxiliary tasks and \textit{without} pretraining:
\begin{enumerate}
    \item \textbf{Whether invariant or equivariant models perform better?} Across 8 tasks, equivariant models such as EGNN, GCPNet and TFN attain the best performance on 5.
    Notably, sequence-based ESM augmented with our structural featurisation matches state-of-the-art protein-specific GNNs \citep{fan2023continuousdiscrete} on (super)family and gene ontology prediction.
    \item \textbf{Which input representation is the best for each respective task?} Featurising models with \caa atoms, virtual angles, and backbone torsions provides the best performance overall on 21 out of 48 combinations of models and tasks. This suggests that letting models implicitly learn about side chain orientation and flexibility by using backbone-only featurisation may prevent overfitting on crystallisation artifacts \citep{Dauparas2022}.
    \item \textbf{Whether auxiliary denoising tasks improve model performance?} Sequence denoising is a particularly useful auxiliary task for training protein structure encoders until sufficient structural detail makes the task trivial, improving results over not using auxiliary tasks for 98 out of 210 combinations of models, task, and auxiliary tasks.
    Notably, structure denoising helped stabilise the training of MACE models on the GO-CC and Reaction tasks, where other runs did not converge.
\end{enumerate}

% We first set out to determine (1) whether invariant or equivariant models perform better on our set of tasks, (2) which input representation is the best for each respective task and (3) whether auxiliary denoising tasks improve model performance. Table \ref{tab:baseline_graph_classification_results} shows that (1) equivariant models such as EGNN and GCPNet produce the best global and local representations for fold classification and inverse folding, respectively, whereas sequence-based models (i.e., ESM) produce the best representations for (super)family classification. (2) Notably, featurising models with C$\alpha$ atoms and virtual angles provide the best performance overall. The same set of results also suggests that (3) sequence denoising is a particularly useful auxiliary task for training protein structure encoders.

\subsection{Incorporating More Structural Detail Improves Pre-Training Performance}

We then investigated protein structure pre-training in Table  \ref{tab:pre-training} to find out:
\begin{enumerate}
    \item \textbf{Which input representation is best for pre-training?} Incorporating dihedral angles consistently improves validation metrics on pre-training tasks, more so than architecture.
    \item \textbf{Which GNNs benefit from which pre-training task?} Inverse folding, sequence denoising, and torsional denoising benefit equivariant models the most in the context of pre-training, whereas pLDDT prediction and structure denoising benefit invariant models the most, suggesting that certain pre-training tasks benefit certain classes of models more than other tasks.
    Unfortunately, we were currently unable to pre-train spherical equivariant GNNs (TFN, MACE) due to the high computational requirements of these models.
\end{enumerate}

% We then investigated protein structure pre-training to find out (1) which input representation is best for pre-training and (2) which geometric neural networks benefit from which pre-training task. Table \ref{tab:pre-training} shows that (1) incorporating dihedral angles consistently improves validation metrics on pre-training tasks, more so than architecture. These results also suggest that (2) inverse folding, sequence denoising, and torsional denoising benefit equivariant models the most in the context of pre-training, whereas pLDDT prediction and structure denoising benefit invariant models the most, suggesting that certain pre-training tasks benefit certain classes of models more than other tasks.

%%%

\begin{landscape}

\begin{table}[!t]

\caption{
\textbf{Baseline benchmark results without pretraining.} Results for each model and featurisation pair are given as: \colorbox{orange!20}{no auxiliary task} / \colorbox{blue!20}{+sequence denoising} / \colorbox{green!20}{+structure denoising}. Coloured boxes mark the best auxiliary tasks per model and featurisation, \underline{underlined results} denote the best featurisation choice per model, and \textbf{bold results} are the best models for each task. 
\colorbox{gray!20}{Greyed} cells denote invalid task-setup combinations (e.g. inverse folding and sequence denoising as auxiliary task), and ----- denote runs that did not converge. 
[*] denotes results for ESM without structural features, taken from \citet{zhang2023protein}.
\textbf{Key takeaways:} (1) Sequence denoising as an auxiliary task consistently improves performance across models.
(2) Equivariant GNNs outperform invariant GNNs, in general.
(3) \caa, virtual angles, and backbone torsions provide the best featurisation.
(4) Augmenting ESM2 with structural features provides compelling performance for (super)family fold classification and gene ontology prediction compared to GNNs.
% Baseline tasks without pre-training. Results are given as: \colorbox{orange!20}{no auxiliary task} / \colorbox{blue!20}{+sequence denoising} / \colorbox{green!20}{+structure denoising}. Coloured boxes mark the best auxiliary tasks per method and featurisation, underline the best featurisation per method and bold the best method, all on a per-task basis. Greyed cells denote invalid task-setup combinations (e.g. inverse folding and sequence denoising as auxiliary task). Sequence denoising consistently improves performance. %Lines denote configurations that failed to converge after 6 hours.} 
}
\label{tab:baseline_graph_classification_results}

\begin{adjustbox}{max width=\linewidth}
\begin{tabular}{lllcccccccccccccl}
\toprule
\multirow{2}{*}{\textbf{Method}} & \multicolumn{1}{c}{\multirow{2}{*}{\textbf{Features}}} &%\multicolumn{1}{c}{\multirow{2}{*}{\textbf{EC} ($\uparrow$)}} 
\multicolumn{3}{c}{\multirow{2}{*}{\textbf{GO-CC} ($\uparrow$)}} & \multirow{2}{*}{\textbf{Ab. Dev.} ($\uparrow$)} & \multicolumn{3}{c}{\textbf{Fold} ($\uparrow$)} & \multirow{2}{*}{\textbf{Reaction} ($\uparrow$)} & \multirow{2}{*}{\textbf{PPI} ($\uparrow$)} &
%\multirow{2}{*}{\textbf{PTM} ($\uparrow$)} 
%& \multirow{2}{*}{\textbf{PTM} ($\downarrow$)} & 
& &
\multirow{2}{*}{\textbf{Inverse Folding} ($\downarrow$)}\\
% \cmidrule{3-5}
\cmidrule{7-9}
 %& \multicolumn{1}{c}{} & \textbf{BP} & \textbf{MF} 
 & & &
 & & \multicolumn{1}{c}{} & \textbf{Fold} & \textbf{Family} & \textbf{Superfamily} &  &  &  &  &  &  &  &   \\
\midrule 


\multirow{2}{*}{ESM} & Seq. & & & 0.394* & 0.91 & 26.8* & 97.8* & 60.1* & 80.87 & 0.953 & & & N/A\\ 
&  + \virt, \bb &  & & \textbf{0.545$\pm 0.00$}
& 0.91 & 39.89$\pm 0.00$  & \textbf{99.69}$\pm 0.00$ & \textbf{78.68}$\pm 0.00$  & 81.79 & 0.953$\pm0.00$ &  &  & N/A &  &  &  \\


\midrule 

\multirow{5}{*}{SchNet} & \caa &  & 
%49.70 / \colorbox{blue!20}{57.45} / 44.50 
& 0.387$\pm 0.01$ / 0.415$\pm 0.00$ / \colorbox{green!20}{0.418$\pm 0.00$} & \colorbox{orange!20}{0.89$\pm 0.00$} / 0.88$\pm 0.00$ / 0.82$\pm 0.00$ & 27.11$\pm 0.01$ /  \colorbox{blue!20}{31.36$\pm 0.01$} / 26.05$\pm 0.01$& 81.29$\pm 0.02$ / \colorbox{blue!20}{84.88$\pm 0.01$} / 79.03$\pm 0.03$& 30.02$\pm 0.00$  / \colorbox{blue!20}{34.79$\pm 0.01$} / 27.79$\pm 0.01$& 58.94$\pm 0.01$ / \colorbox{blue!20}{68.33$\pm 0.01$} / 57.59$\pm 0.03$ & 0.954$\pm 0.00$ / 0.952$\pm 0.00$ / \colorbox{green!20}{0.955$\pm 0.00$} & &  & \cellcolor{gray!20} &  &  &  \\

 & + Seq. &  & 
 %46.44 / \colorbox{blue!20}{53.83} / 45.25 
 & 0.390$\pm 0.00$ / 0.374$\pm 0.03$ / \colorbox{green!20}{0.421$\pm 0.00$} & 0.85$\pm 0.05$ / 0.86 / \colorbox{green!20}{0.87} & 34.44$\pm 0.00$ / \colorbox{blue!20}{\underline{38.06$\pm 0.01$}} / 33.31$\pm 0.01$& 91.12$\pm 0.02$ / \colorbox{blue!20}{92.59$\pm 0.01$} / 91.82$\pm 0.02$&  39.33$\pm0.02$ / \colorbox{blue!20}{44.27$\pm 0.01$} / 39.67$\pm 0.01$& \colorbox{orange!20}{70.30$\pm 0.01$} / 68.83$\pm 0.01$ / 62.94$\pm 0.10$ & \colorbox{orange!20}{0.953$\pm 0.00$} / 0.950$\pm 0.00$ / 0.953$\pm 0.00$ & & & \colorbox{orange!20}{11.78$\pm 0.08$} /  12.34$\pm 0.11$ &  &  &  \\
 
 & \ + \virt &  & 
 %\colorbox{orange!20}{55.56} / -------- / 50.90 
& 0.403$\pm 0.00$ / 0.379$\pm 0.01$ / \colorbox{green!20}{0.420$\pm 0.00$}  & 0.86 / 0.84 / \colorbox{green!20}{0.88} & 33.22$\pm0.02$ / \colorbox{blue!20}{37.93$\pm 0.00$} / 34.99$\pm 0.01$& 91.33$\pm 0.03$ / \colorbox{blue!20}{\underline{94.06$\pm 0.01$}} / 93.14$\pm 0.01$& 39.96$\pm 0.03$ / \colorbox{blue!20}{\underline{45.61$\pm 0.01$}} / 39.39$\pm 0.02$ & 70.33$\pm 0.01$ / \colorbox{blue!20}{72.58$\pm 0.01$} / 65.93$\pm 0.03$ & 0.953$\pm 0.00$ / 0.951$\pm 0.00$ / \colorbox{green!20}{0.954$\pm 0.00$} & &  & \colorbox{orange!20}{11.03$\pm 0.03$} / 11.81$\pm 0.50$ &  &  &  \\
 
 & \ \ + \bb &  &
 %54.81 / \colorbox{blue!20}{\underline{61.23}} / 54.81
 & 0.412$\pm 0.01$ / 0.406$\pm 0.01$ / \colorbox{green!20}{\underline{0.429$\pm 0.00$}} &  0.85 / \colorbox{blue!20}{\underline{0.90}} / 0.86 & \colorbox{orange!20}{36.01$\pm 0.01$} / 35.88$\pm 0.01$ / 33.71$\pm 0.02$ & \colorbox{orange!20}{93.42$\pm 0.01$} / 93.13$\pm 0.00$ / 92.84$\pm 0.02$& 42.97$\pm 0.01$ / \colorbox{blue!20}{44.14$\pm 0.01$} / 41.33 $\pm 0.03$& 67.53$\pm 0.03$ / \colorbox{blue!20}{\underline{73.83$\pm 0.02$}} / 72.28$\pm 0.01$ & 0.954$\pm 0.00$ / 0.952$\pm 0.00$ / \colorbox{green!20}{0.956$\pm 0.00$} & & & \colorbox{orange!20}{\underline{9.97$\pm 0.09$}} / 10.76$\pm 0.02$  &  &  &  \\
 
 & \ \ \ + \schi &  & 
 %55.21 / \colorbox{blue!20}{57.89} / -------- 
 & 0.390$\pm 0.00$ / 0.392$\pm 0.01$ / \colorbox{green!20}{0.423$\pm 0.00$} & 0.87 / \colorbox{blue!20}{0.89} / 0.87 & 35.60$\pm 0.02$ / 33.90 $\pm 0.02$ / \colorbox{green!20}{35.79$\pm 0.02$} & 92.96$\pm 0.00$ / 90.80$\pm 0.01$ / \colorbox{green!20}{93.05$\pm 0.00$}& \colorbox{orange!20}{43.05$\pm 0.01$} / 40.77$\pm 0.01$ / 41.72 $\pm 0.02$& 68.72$\pm 0.03$ / 68.87$\pm 0.00$ / \colorbox{green!20}{71.40$\pm 0.01$} & 0.954$\pm 0.00$ / 0.951$\pm 0.00$ / \colorbox{green!20}{\underline{0.955}$\pm 0.00$}  &  &  & \cellcolor{gray!20} &  &  &  \\
 
\midrule
%\multicolumn{1}{l}{\multirow{5}{*}{DimeNet}} & \caa &  &
%-------- / -------- / 27.99 
%& & 16.34 / \colorbox{blue!20}{18.37} / 15.06 & 71.87 / \colorbox{blue!20}{74.08} / 55.37 & 21.92 / \colorbox{blue!20}{23.36} /15.47 &  &  & 95.54 / \colorbox{green!20}{\underline{95.66}} &  \cellcolor{gray!20}&  &  &  \\

%\multicolumn{1}{l}{} & \caa + Seq. &  & 
%-------- / -------- / 17.72 
%&  & 16.30 / \colorbox{blue!20}{20.31} / 15.18 & 66.64 / \colorbox{blue!20}{72.99} / 48.42 & 20.36 / \colorbox{blue!20}{25.32} / 13.54 &  &  & 95.49 / \colorbox{green!20}{95.61}  & 10.64 / -------- &  &  &  \\

%\multicolumn{1}{l}{} & \caa + \virt &  & 
%-------- / -------- / 30.99
%&  & \colorbox{orange!20}{18.14} / 16.36 / 16.14 & \colorbox{orange!20}{70.45} / 62.38 / 62.06 & \colorbox{orange!20}{21.04} / 19.51 / 17.79 & &  & \colorbox{orange!20}{95.53} / 95.45 & \colorbox{orange!20}{10.18} /   &  %&  &  \\

%\multicolumn{1}{l}{} & \caa + \virt + \bb &  & 
%-------- / -------- / 31.00
%&  & 
%18.39 / \colorbox{blue!20}{\underline{21.65}} / -------- & 72.94 / \colorbox{blue!20}{\underline{77.14}} / -------- & 23.23 / \colorbox{blue!20}{\underline{25.36}} / -------- &  &  &  95.52 / \colorbox{green!20}{95.60}  & \underline{\textbf{9.91}} / -------- &  &  \\

%\multicolumn{1}{l}{} & \caa + \virt + \bb + \schi &  & 
%-------- / -------- / -------- 
%&  &  16.83 / \colorbox{blue!20}{19.73} / -------- & 69.67 / \colorbox{blue!20}{75.59} / -------- & 22.03 / \colorbox{blue!20}{23.88} / -------- &  &  & 95.42 / \colorbox{green!20}{95.52} &  \cellcolor{gray!20} &  &  &  \\
%\midrule

\multicolumn{1}{l}{\multirow{5}{*}{GearNet-Edge}} & \caa & &  & 0.450$\pm 0.00$ / 0.408$\pm 0.07$ / \colorbox{green!20}{\underline{0.453$\pm 0.00$}} & 0.82 / \colorbox{blue!20}{0.83} / 0.80 & 38.14$\pm 0.03$ / 38.15 $\pm 0.02$  / \colorbox{green!20}{38.62$\pm 0.01$}& 95.45$\pm 0.02$ / \colorbox{blue!20}{96.15$\pm 0.01$} / 95.73$\pm 0.02$ & 51.16$\pm 0.03$ /\colorbox{blue!20}{53.52 $\pm 0.04$} / 52.24$\pm 0.05$  & 78.14$\pm 0.01$ / \colorbox{blue!20}{\underline{80.03$\pm 0.01$}} / 79.12$\pm 0.0$ & 0.959$\pm 0.00$ / 0.950$\pm 0.00$ / \colorbox{green!20}{0.961$\pm 0.00$} & & &  \cellcolor{gray!20} &  & \\

\multicolumn{1}{l}{} & + Seq. & &  & 0.437$\pm 0.00$ / \colorbox{blue!20}{0.439$\pm 0.01$} / 0.400$\pm 0.07$  & 0.80 / \colorbox{blue!20}{\underline{0.84}} / 0.81  & -------------- / 38.42$\pm 0.01$ / \colorbox{green!20}{39.45$\pm 0.02$}& -------------- / 95.35$\pm 0.02$ / \colorbox{green!20}{96.45$\pm 0.00$}  & -------------- / 50.98$\pm 0.02$ / \colorbox{green!20}{52.35$\pm 0.01$} & 77.02$\pm 0.03$ / \colorbox{blue!20}{77.74$\pm 0.03$} / 75.80$\pm 0.03$ &  \colorbox{orange!20}{0.956$\pm 0.00$} / 0.956$\pm 0.00$ / 0.956$\pm 0.00$ &  &  & 12.79$\pm 0.17$ / \colorbox{green!20}{12.60$\pm 0.16$}&  &  &  \\

\multicolumn{1}{l}{} & \ + \virt & &  & 0.436$\pm 0.00$ / 0.431$\pm 0.00$ / \colorbox{green!20}{0.441$\pm 0.00$}  & \colorbox{orange!20}{0.82} / 0.81 / 0.82 & -------------- / 40.69$\pm 0.01$ / \colorbox{green!20}{\underline{41.52$\pm 0.00$}} & -------------- / \colorbox{blue!20}{\underline{97.12$\pm 0.00$}} / 96.57$\pm 0.00$& -------------- / \colorbox{blue!20}{\underline{54.83$\pm 0.02$}} / 53.32$\pm 0.00$ & 77.45$\pm 0.01$ / \colorbox{blue!20}{77.76$\pm 0.01$} / 76.88$\pm 0.00$ & \colorbox{orange!20}{0.958$\pm 0.00$} / 0.955$\pm 0.00$ / 0.957$\pm 0.00$  &  &  & 12.35$\pm 0.05$ / \colorbox{green!20}{11.91 $\pm 0.13$}\\

\multicolumn{1}{l}{} & \ \ + \bb & &  & 0.441$\pm 0.00$ / \colorbox{blue!20}{0.443$\pm 0.00$} / 0.442$\pm 0.01$ & \colorbox{orange!20}{0.81} / 0.80 / 0.78 & 39.92$\pm 0.01$ / 41.31$\pm 0.00$ / \colorbox{green!20}{\underline{41.52 $\pm 0.00$}} & 96.26$\pm 0.00$ / 96.68$\pm 0.00$ / \colorbox{green!20}{96.87 $\pm 0.00$} & 53.00$\pm 0.03$ / 54.12$\pm 0.02$ / \colorbox{green!20}{54.72$\pm 0.01$} & 76.61$\pm 0.01$ / \colorbox{green!20}{78.20$\pm 0.01$} / 76.57$\pm 0.02$ & 0.954$\pm 0.01$ / 0.956$\pm 0.00$ / \colorbox{green!20}{0.960$\pm 0.00$} & & & 11.61$\pm 0.12$ / \colorbox{green!20}{\underline{11.23$\pm 0.09$}}&  &  \\

\multicolumn{1}{l}{} & \ \ \ + \schi &&   & 0.430$\pm 0.01$ / \colorbox{blue!20}{0.437$\pm 0.00$} / 0.437$\pm 0.00$ & 0.81 / \colorbox{blue!20}{0.83} / 0.83 & 37.59$\pm 0.02$ / \colorbox{blue!20}{39.23$\pm 0.01$} / 39.11$\pm 0.00$& 96.09$\pm 0.01$ / 95.71$\pm 0.00$ / \colorbox{green!20}{96.10$\pm 0.00$} & \colorbox{orange!20}{51.10$\pm 0.01$} / 50.23$\pm 0.00$ / 50.86$\pm 0.00$ & 75.91$\pm0.03$ / 74.14$\pm 0.01$ / \colorbox{green!20}{77.76$\pm 0.01$} & 0.959$\pm 0.00$ / 0.958$\pm 0.00$ / \colorbox{green!20}{0.962$\pm 0.00$} & & & \cellcolor{gray!20}  &  &  &   \\
\midrule



\multicolumn{1}{l}{\multirow{5}{*}{EGNN}} & \caa & &  & 0.400$\pm 0.00$ / \colorbox{blue!20}{0.454$\pm 0.01$} / 0.399$\pm 0.01$  & \colorbox{orange!20}{\underline{0.93}} / 0.90 / 0.92 & 31.51$\pm 0.01$ / \colorbox{blue!20}{36.81$\pm 0.01$} / 30.84$\pm 0.01$ & 94.54$\pm 0.00$ / \colorbox{blue!20}{96.71$\pm0.00$} / 94.14$\pm 0.00$& 42.70$\pm 0.02$ / \colorbox{blue!20}{50.93$\pm 0.01$} / 41.87$\pm 0.01$& 65.78$\pm 0.01$ /\colorbox{blue!20}{81.59$\pm 0.01$} / 64.53$\pm 0.03$ & \colorbox{orange!20}{\underline{0.965$\pm 0.00$}} / 0.964$\pm 0.00$ / 0.964$\pm 0.00$ & &  & \cellcolor{gray!20} &  &  &  \\

\multicolumn{1}{l}{} & + Seq. & &  & 0.390$\pm 0.01$ / \colorbox{blue!20}{0.450$\pm 0.01$} / 0.372$\pm 0.01$  & 0.87 / \colorbox{blue!20}{0.90} / 0.88 & 41.63$\pm 0.01$ / \colorbox{blue!20}{44.77$\pm 0.01$} / 42.68$\pm 0.00$ & 97.87$\pm 0.00$ / \colorbox{blue!20}{98.00$\pm 0.00$} / 97.89$\pm 0.0$0& 55.47$\pm 0.00$ / \colorbox{blue!20}{59.54$\pm 0.01$} / 56.80$\pm 0.03$ & 74.36$\pm 0.01$ / \colorbox{blue!20}{77.46$\pm 0.01$} / 74.38$\pm 0.01$ &  \colorbox{orange!20}{0.962$\pm 0.00$} / 0.962$\pm 0.00$ / 0.960$\pm 0.00$ &  &  & \colorbox{orange!20}{10.28$\pm 0.04$} / 10.53$\pm 0.01$  &  &  &  \\

\multicolumn{1}{l}{} & \ + \virt & &  & 0.409$\pm 0.02$ / \colorbox{blue!20}{\underline{0.455$\pm 0.01$}} / 0.401$\pm 0.00$ & 0.88 / \colorbox{blue!20}{0.89} / 0.86 & 45.09$\pm 0.00$ / \colorbox{blue!20}{\underline{\textbf{48.26}$\pm 0.02$}} / 43.65$\pm 0.00$& 98.09$\pm 0.00$ / \colorbox{blue!20}{\underline{98.55$\pm 0.00$}} / 98.03$\pm 0.00$& 58.15$\pm 0.01$ / \colorbox{blue!20}{\underline{62.86$\pm0.01$}} / 58.31$\pm 0.01$& 78.97$\pm 0.01$ / \colorbox{blue!20}{\underline{\textbf{82.70}$\pm 0.00$}} / 78.24$\pm 0.01$  &  \colorbox{orange!20}{0.965$\pm 0.00$} / 0.963$\pm 0.00$ / 0.964$\pm 0.00$ & & &  \colorbox{orange!20}{9.84$\pm 0.07$} / 10.07$\pm 0.04$ &  &  &  \\

\multicolumn{1}{l}{} & \ \ + \bb &  & & 0.396$\pm 0.01$ / \colorbox{blue!20}{0.431$\pm 0.03$} / 0.389$\pm 0.01$ & \colorbox{orange!20}{0.88} / 0.87 / 0.88 & 44.70$\pm 0.01$ / \colorbox{blue!20}{46.97$\pm 0.01$} / 45.49$\pm 0.01$ & 97.76$\pm 0.00$ / 97.99$\pm 0.00$ / \colorbox{green!20}{98.00$\pm 0.00$} & 59.66$\pm 0.01$ / \colorbox{blue!20}{62.83$\pm 0.01$} / 60.38$\pm 0.02$& 78.01$\pm 0.02$ / \colorbox{blue!20}{82.12$\pm 0.01$} / 78.74$\pm 0.01$ & \colorbox{orange!20}{0.964$\pm 0.00$} / 0.962$\pm 0.00$ / 0.963$\pm 0.00$ & & & \colorbox{orange!20}{\underline{8.89$\pm 0.04$}} / 9.65$\pm 0.03$&  &  &  \\

\multicolumn{1}{l}{} & \ \ \ + \schi &  & & 0.406$\pm 0.02$ / \colorbox{blue!20}{0.432$\pm 0.00$} / 0.397$\pm 0.02$ & 0.86 / \colorbox{blue!20}{0.90} / 0.87 & 43.16$\pm 0.01$ / \colorbox{blue!20}{44.07$\pm 0.00$} / 43.41$\pm 0.02$& \colorbox{orange!20}{97.78$\pm 0.00$} / 97.46$\pm 0.00$ / 97.76$\pm 0.00$ & \colorbox{orange!20}{59.01$\pm 0.01$} / 56.94$\pm 0.01$ / 58.4$\pm 0.01$& 76.98$\pm 0.01$ / \colorbox{blue!20}{80.43$\pm 0.01$} / 75.20$\pm 0.07$ & \colorbox{orange!20}{0.963$\pm 0.00$} / 0.962$\pm 0.00$ / 0.961$\pm 0.00$ & & & \cellcolor{gray!20} &  &  &   \\
\midrule


\multicolumn{1}{l}{\multirow{5}{*}{GCPNet}} & \caa &  & 
%/ -------- / 49.72 
& 0.415$\pm 0.01$ / \colorbox{blue!20}{0.435$\pm 0.01$} / 0.430$\pm 0.00$ & 0.82 / \colorbox{blue!20}{\underline{0.90}} / 0.84 & 38.40$\pm 0.00$ / \colorbox{blue!20}{43.11$\pm0.01$} / 37.43$\pm 0.01$ & 95.75$\pm 0.00$ / \colorbox{blue!20}{97.22$\pm0.00$} / 95.57$\pm 0.01$ & 47.85$\pm 0.02$ / \colorbox{blue!20}{56.47$\pm 0.00$} / 47.55$\pm 0.01$ & 66.97$\pm 0.01$ / \colorbox{blue!20}{76.47$\pm 0.00$} / 68.32$\pm 0.02$ & \colorbox{orange!20}{\underline{\textbf{0.968$\pm 0.00$}}} / 0.960$\pm 0.00$ / 0.967$\pm 0.00$ & &  & \cellcolor{gray!20} &  &  &  \\

\multicolumn{1}{l}{} & + Seq. &  & 
%/ -------- / 51.77
& 0.391$\pm 0.00$ / \colorbox{blue!20}{0.423$\pm 0.03$} / 0.389$\pm 0.00$ & 0.84 / \colorbox{blue!20}{0.85} / 0.84 & 43.89$\pm 0.01$ / \colorbox{blue!20}{45.38$\pm 0.01$} /  42.33$\pm 0.00$ & 97.31$\pm 0.00$ / \colorbox{blue!20}{97.49$\pm 0.00$} / 97.19$\pm 0.00$ & 54.19$\pm 0.01$ / \colorbox{blue!20}{\underline{58.89$\pm 0.02$}}  / 53.07$\pm 0.00$ & \colorbox{orange!20}{73.00$\pm 0.02$} / 72.18$\pm 0.02$ / 72.59$\pm 0.02$ & \colorbox{orange!20}{0.968$\pm 0.00$} / 0.961$\pm 0.00$ / 0.967$\pm 0.00$ &  &  &  \colorbox{orange!20}{8.35$\pm 0.08$} / 8.92$\pm 0.07$ &  &  &  \\

\multicolumn{1}{l}{} & \ + \virt &  &  & 0.427$\pm 0.00$ / \colorbox{blue!20}{\underline{0.442$\pm 0.01$}} / 0.422$\pm 0.00$ & \colorbox{orange!20}{0.88} / 0.82 / 0.85 &  43.92$\pm 0.02$ / \colorbox{blue!20}{\underline{45.71$\pm 0.00$}} / 43.76$\pm 0.01$ & 97.93 $\pm 0.00$ / 97.85 $\pm 0.00$ / \colorbox{green!20}{\underline{97.95$\pm 0.00$}} & 56.28$\pm 0.01$ / \colorbox{blue!20}{58.61$\pm 0.00$} / 57.91$\pm 0.00$ & 76.46$\pm 0.01$ / \colorbox{blue!20}{76.89$\pm 0.01$} / 75.46$\pm 0.01$ & 0.966$\pm 0.00$ / 0.962$\pm 0.00$ / \colorbox{green!20}{0.967$\pm 0.00$} & & &  \colorbox{orange!20}{8.80$\pm 0.09$} / 9.49$\pm 0.18$ &  &  &  \\

\multicolumn{1}{l}{} & \ \ + \bb &  & & 0.424$\pm 0.00$ / \colorbox{blue!20}{0.436$\pm 0.01$} / 0.426$\pm 0.01$  & \colorbox{orange!20}{0.87} / 0.83 / 0.87 & 44.76$\pm 0.01$ / \colorbox{blue!20}{45.52$\pm 0.01$} / 45.41$\pm 0.00$ & 97.55$\pm 0.00$ / 97.33$\pm 0.00$ / \colorbox{green!20}{97.56$\pm 0.00$} & 56.83$\pm 0.01$ / \colorbox{blue!20}{57.97$\pm 0.00$} / 57.33$\pm 0.01$ & 75.49$\pm 0.01$ / 77.01$\pm 0.01$ / \colorbox{green!20}{\underline{77.71$\pm 0.01$}} & \colorbox{orange!20}{0.967$\pm 0.00$} / 0.962$\pm 0.00$ / 0.967$\pm 0.00$ &  &   &  \colorbox{orange!20}{\underline{\textbf{7.56}$\pm 0.11$}} / 8.60$\pm 0.09$ &  &  &  \\

\multicolumn{1}{l}{} & \ \ \ + \schi &  & & 0.410$\pm 0.01$ / \colorbox{blue!20}{0.431$\pm 0.00$} / 0.421$\pm 0.00$ & 0.85 / \colorbox{blue!20}{0.86} / 0.80 & 42.01$\pm 0.02$ / 42.52$\pm 0.01$ / \colorbox{green!20}{43.6$\pm 0.01$} & 96.68$\pm 0.00$ / 96.55$\pm 0.00$ / \colorbox{green!20}{97.36$\pm 0.00$} & 53.4$\pm 0.01$ / 53.14$\pm 0.01$ / \colorbox{green!20}{54.55$\pm 0.00$} & 73.00$\pm 0.03$ / \colorbox{blue!20}{74.35$\pm 0.02$} / 71.78$\pm 0.04$ & \colorbox{orange!20}{\underline{\textbf{0.968$\pm 0.00$}}} / 0.964$\pm 0.00$ / 0.968$\pm 0.00$ &  &  & \cellcolor{gray!20} & &  &    \\

\midrule

\multicolumn{1}{l}{\multirow{5}{*}{TFN}} & \caa &  & & 0.421$\pm 0.00$ / \colorbox{blue!20}{0.452$\pm 0.00$} / 0.429$\pm 0.00$ & \colorbox{orange!20}{\underline{\textbf{0.94}}} / 0.90 / \underline{\textbf{0.94}} & 31.16$\pm 0.00$ / \colorbox{blue!20}{37.81$\pm 0.00$} / --------------& 94.59$\pm 0.00$ / \colorbox{blue!20}{97.35$\pm 0.00$} / -------------- & 41.61 $\pm 0.00$ / \colorbox{blue!20}{53.37$\pm 0.01$} / -------------- & 69.22$\pm 0.02$ / \colorbox{blue!20}{\underline{81.22$\pm 0.01$}} / 67.67$\pm 0.01$ & \colorbox{orange!20}{0.967$\pm 0.00$} / 0.962$\pm 0.00$ / 0.966$\pm 0.00$ &  &  & \cellcolor{gray!20}  &  &  &  \\

\multicolumn{1}{l}{} & + Seq. &  &  & 0.396$\pm 0.00$ / \colorbox{blue!20}{0.435$\pm 0.00$} / 0.402$\pm 0.00$ & \colorbox{orange!20}{0.88} / 0.88 / ------- & 36.80$\pm 0.00$ / \colorbox{blue!20}{39.33$\pm 0.00$} / 38.80$\pm 0.00$ & 96.14 $\pm 0.00$ / \colorbox{blue!20}{96.72$\pm 0.00$} / 96.38$\pm 0.00$& 48.40$\pm 0.01$ / \colorbox{blue!20}{52.48$\pm 0.01$} / 48.86$\pm 0.00$ & \colorbox{orange!20}{73.69$\pm 0.01$} / 71.27$\pm 0.02$ / 69.58$\pm 0.04$ & 0.961$\pm 0.00$ / 0.960$\pm 0.00$ / \colorbox{green!20}{0.961$\pm 0.00$} &  &  &  \colorbox{orange!20}{10.34$\pm 0.03$} / 10.84$\pm 0.04$&  &  &  \\

\multicolumn{1}{l}{} & \ + \virt &  & & 0.408$\pm 0.00$ / \colorbox{blue!20}{\underline{0.438$\pm 0.00$}} / 0.418$\pm 0.00$ & \colorbox{orange!20}{0.90} / ------- / ------- & 38.89$\pm 0.01$ /\colorbox{blue!20}{42.56$\pm 0.00$} / 40.93 $\pm 0.00$ & 97.41 $\pm 0.00$ / \colorbox{blue!20}{\underline{97.84 $\pm 0.00$}} / 97.59 $\pm 0.00$  & 53.49$\pm 0.01$ / \colorbox{blue!20}{57.54 $\pm 0.00$} / 54.27$\pm 0.00$ & 75.39$\pm 0.02$ / \colorbox{blue!20}{78.54$\pm 0.01$} / 75.67$\pm 0.02$ & \colorbox{orange!20}{0.965$\pm 0.00$} / 0.961$\pm 0.00$ / 0.964$\pm 0.00$ &  &  & \colorbox{orange!20}{10.02$\pm 0.05$} / 10.46$\pm 0.11$ &  &  &  \\

\multicolumn{1}{l}{} & \ \ + \bb &  & & 0.405$\pm 0.00$ / \colorbox{blue!20}{0.449$\pm 0.00$} / 0.417$\pm 0.01$ & \colorbox{orange!20}{0.90} / 0.89 / 0.88 & 41.09$\pm 0.00$ / \colorbox{blue!20}{\underline{43.11$\pm 0.00$}} / 39.68$\pm 0.00$ & 97.65$\pm 0.00$ / \colorbox{blue!20}{97.66 $\pm 0.00$} / 97.63 $\pm 0.00$ & 54.00 $\pm 0.00$ / \colorbox{blue!20}{\underline{59.65 $\pm 0.00$}} / 56.19$\pm 0.00$ & \colorbox{orange!20}{80.84$\pm 0.01$} / 76.17$\pm 0.02$ / 76.12$\pm 0.02$  & \colorbox{orange!20}{\underline{0.967$\pm 0.00$}} / 0.963$\pm 0.00$ / 0.967$\pm 0.00$ &  &  &  \colorbox{orange!20}{\underline{8.73$\pm 0.02$}} / 9.73$\pm 0.01$&  &  &  \\

\multicolumn{1}{l}{} & \ \ \  + \schi &  & & 0.407$\pm 0.01$ / \colorbox{blue!20}{0.429$\pm 0.00$} / 0.418$\pm 0.00$  & \colorbox{orange!20}{0.88} / ------- / ------- &  \colorbox{orange!20}{40.07$\pm 0.00$} / 38.33 $\pm 0.00$ / --------------& \colorbox{orange!20}{97.36$\pm 0.00$} / 96.96 $\pm 0.00$ / --------------& 54.79$\pm 0.00$ / \colorbox{blue!20}{55.14$\pm 0.00$} / --------------& \colorbox{orange!20}{78.22$\pm 0.01$} / 74.58$\pm 0.01$ / 72.68$\pm 0.01$ & \colorbox{orange!20}{0.965$\pm 0.00$} / 0.962$\pm 0.00$ / 0.964$\pm 0.00$ &  &  & \cellcolor{gray!20} &  &  &   \\
\midrule

\multicolumn{1}{l}{\multirow{5}{*}{MACE}} & \caa &  & & ------- / ------- / \colorbox{green!20}{\underline{0.411$\pm 0.01$}}  & \colorbox{orange!20}{\underline{0.93}} / 0.92 / \underline{0.93} & 34.31$\pm 0.01$ / \colorbox{blue!20}{36.68$\pm 0.00$} / 35.21$\pm 0.01$ & 93.10$\pm 0.01$ / \colorbox{blue!20}{95.67$\pm 0.01$} / 92.4$\pm 0.00$& 44.86$\pm 0.02$ / \colorbox{blue!20}{50.41$\pm 0.02$} / 44.14$\pm 0.02$ & ------- / ------- / \colorbox{green!20}{62.38$\pm 0.01$} & \colorbox{orange!20}{0.964$\pm 0.00$} / 0.960$\pm 0.00$ / 0.964$\pm 0.00$ &  &  & \cellcolor{gray!20} &  &  &  \\

\multicolumn{1}{l}{} & + Seq. &  &  & ------- / ------- / \colorbox{green!20}{0.394$\pm 0.01$} & 0.82 / \colorbox{blue!20}{0.89} / 0.84 & 38.23$\pm 0.02$ / \colorbox{blue!20}{40.23$\pm 0.01$} / 38.75$\pm 0.00$ & 96.06$\pm 0.01$ / \colorbox{blue!20}{96.21$\pm 0.00$} / 95.62$\pm 0.00$ & 48.45$\pm 0.03$ / \colorbox{blue!20}{50.75$\pm 0.01$} / 48.24$\pm 0.02$& ------- / ------- / \colorbox{green!20}{69.32$\pm 0.01$} & \colorbox{orange!20}{0.964$\pm 0.00$} / 0.960$\pm 0.00$ / 0.963$\pm 0.00$ & &  & \colorbox{orange!20}{9.95$\pm 0.10$} / 10.3$\pm 0.04$ &  &  &  \\

\multicolumn{1}{l}{} & \ + \virt & & & ------- / ------- / \colorbox{green!20}{0.393$\pm 0.00$} & 0.72 / \colorbox{blue!20}{0.90} / 0.78 & 39.20$\pm 0.02$ / \colorbox{blue!20}{40.39$\pm 0.01$} / 39.02$\pm 0.02$ & 95.92$\pm 0.01$ / \colorbox{blue!20}{96.78$\pm 0.01$} / 96.15$\pm 0.01$ & 50.16$\pm 0.01$ / \colorbox{blue!20}{53.32$\pm 0.01$} / 51.46$\pm 0.02$ & ------- / ------- / \colorbox{green!20}{74.34$\pm 0.02$} & \colorbox{orange!20}{0.965$\pm 0.00$} / 0.960$\pm 0.00$ / 0.963$\pm 0.00$ &  &  & \colorbox{orange!20}{10.30$\pm 0.02$} / 10.60$\pm 0.02$ &  &  &  \\

\multicolumn{1}{l}{} & \ \ + \bb & & & ------- / ------- / \colorbox{green!20}{0.404$\pm 0.00$} & 0.83 / \colorbox{blue!20}{0.89} / 0.84 & \colorbox{orange!20}{\underline{42.17$\pm 0.02$}} / 40.55 $\pm 0.02$ / 41.29$\pm 0.02$ & \colorbox{orange!20}{\underline{97.53$\pm 0.01$}} / 97.23$\pm 0.01$ / 97.34 $\pm 0.00$ & \colorbox{orange!20}{\underline{55.24$\pm 0.02$}} / 55.15$\pm 0.01$ / 54.17$\pm 0.01$ & ------- / ------- / \colorbox{green!20}{\underline{76.34$\pm 0.01$}} &  \colorbox{orange!20}{\underline{0.965$\pm 0.00$}} / 0.960$\pm 0.00$ / 0.964$\pm 0.00$ &  &  &  \colorbox{orange!20}{\underline{8.94$\pm 0.03$}} / 9.75$\pm 0.07$ &  &  &  \\

\multicolumn{1}{l}{} & \ \ \ + \schi & &  & ------- / ------- / \colorbox{green!20}{0.397$\pm 0.01$} & 0.82 / \colorbox{blue!20}{0.88} / 0.80 & 39.931$\pm 0.01$ / 39.62$\pm 0.01$ / \colorbox{green!20}{41.74 $\pm 0.02$} & 97.33$\pm 0.00$ / 96.13$\pm 0.00$ / \colorbox{green!20}{97.49$\pm 0.00$}& 54.65$\pm 0.02$ / 51.27$\pm 0.01$ / \colorbox{green!20}{55.05 $\pm 0.01$} & ------- / ------- / \colorbox{green!20}{76.10$\pm 0.02$} & \colorbox{orange!20}{0.965$\pm 0.00$} / 0.960$\pm 0.00$ / 0.964$\pm 0.00$&  &  &\cellcolor{gray!20}  &  &  &   \\

\bottomrule
\end{tabular}

\end{adjustbox}
\end{table}

\end{landscape}

%%%


%%%

\begin{table}[!ht]
    \centering
    \caption{
    \textbf{Validation performance for pretraining tasks on AlphaFoldDB.} Metrics: \text{Inverse Folding}: perplexity; \text{pLDDT, Structure Denoising, Torsional Denoising}: RMSE; \text{Seq. Denoising}: Accuracy.
    \textbf{Key takeaway: }Incorporating backbone geometry consistently improves pretraining performance.
    % Validation performance for pre-training tasks on \texttt{afdb\_rep\_v4}. Incorporating backbone geometry consistently improves pre-training performance. \textbf{Inverse Folding}: perplexity, \textbf{pLDDT, Structure Denoising, Torsional Denoising}: RMSE, \textbf{Seq. Denoising}: Accuracy.
    } 
    \label{tab:pre-training}
    \begin{adjustbox}{max width=\linewidth}
    \begin{tabular}{ccccccccccccccccccc}
         \toprule
         & \multirow{2}{*}{\bf{Method}} & &
            \multicolumn{5}{c}{\bf{Task}}& &
            \\
            \cmidrule{4-8}
             & & & \bf{Inverse Folding} ($\downarrow$) & \bf{pLDDT Pred.} ($\downarrow$) & \bf{Structure Denoising} ($\downarrow$)& \bf{Seq. Denoising} ($\uparrow$) & \bf{Torsional Denoising} ($\downarrow$) \\
         \midrule
         \multirow{4}{*}{\rotatebox{90}{\small {$C_\alpha$ + \virt }}} 
         %\multirow{5}{*}{\rotatebox{90}{\small {$C_\alpha$ + \virt }}} 
         & SchNet & & 7.791 & 0.2397 & 0.0704 & 36.81 & \underline{0.0586}\\
         %& DimeNet & & \textbf{6.016} &  \textbf{0.2100} & \textbf{0.0655} & \textbf{47.07} & -------- \\
         & GearNet-Edge & & 6.596 & 0.2326 & \underline{0.0672} & 43.76 & 0.0615 \\
         & EGNN & & \textbf{6.016} & 0.2406 & 0.0700 & 40.51 & \underline{0.0586}\\
         & GCPNet & & 6.243 & 0.2395 & 0.0679 & \underline{44.81} & \textbf{0.0562}\\
         \midrule
         \multirow{4}{*}{\rotatebox{90}{\small {$C_\alpha$ + $\phi, \psi, \omega$ }}} 
         %\multirow{5}{*}{\rotatebox{90}{\small {$C_\alpha + \phi, \psi, \omega$ }}} 
         & SchNet & & 5.562 & \underline{0.2388} & 0.0603 & 45.61 & 0.0489\\
         %& DimeNet & & 5.962 & \textbf{0.2094} & \textbf{0.0543} & 46.70 & -------- \\
         & GearNet-Edge & & \underline{5.324} & 0.2402 & 0.0562 & 50.15 & 0.0538 \\
         & EGNN & & 5.962 & 0.2403  & 0.0593 & \underline{53.80} & \underline{0.0487}\\
         & GCPNet & & \textbf{3.839} & 0.2399 & \underline{0.0561} & \textbf{59.54} & \textbf{0.0443} \\
        \bottomrule
         
    \end{tabular}
    \end{adjustbox}   
\end{table}

%%%

%%%

\subsection{Pre-training and Greater Structural Detail Benefit Downstream Tasks}

Following the observation that more fine-grained input representations improve pretraining performance, Table \ref{tab:pre_trained_graph_classification_results} explores:
\begin{itemize}
    \item \textbf{Whether these lessons from pretraining translate to downstream tasks?} Equivariant models benefit the most from pretraining on structure-based tasks, particularly when provided the greatest amount of structural detail.
    \item \textbf{Which combination of parameters performs best on downstream tasks?} Providing a greater amount of structural detail compared to a strict C$\alpha$ atom representation benefits downstream performance for \textit{both} invariant and equivariant models.
    Notably, structure denoising consistently improves downstream performance for \textit{both} types of models.
\end{itemize}

%%%

\begin{table}[!ht]
\caption{\textbf{Pretrained model benchmark results.} Results for each model and featurisation pair are given as: \colorbox{orange!20}{no pretraining} / \colorbox{blue!20}{sequence denoising} / \colorbox{green!20}{structure denoising}% / \colorbox{purple!20}{torsion denoising}
, except for inverse folding on CATH, which is pretrained with \colorbox{purple!20}{inverse folding} on AFDB. 
% Coloured boxes mark the best pretraining tasks per model and featurisation, underlined results denote the best featurisation choice per model, and bold results are the best model for each task. 
\textbf{Key takeaway:} Equivariant models benefit most from pretraining and maximum structural detail.
}
\label{tab:pre_trained_graph_classification_results}

\begin{adjustbox}{max width=\linewidth}
\begin{tabular}{cllcccccccc}

\toprule

\multirow{2}{*}{\textbf{Method}} & \multicolumn{1}{c}{\multirow{2}{*}{\textbf{Features}}} & \multicolumn{1}{c}{} & 
%\multicolumn{1}{c}{\multirow{2}{*}{\textbf{EC} ($\uparrow$)}} 
& & \multicolumn{3}{c}{\textbf{Fold} ($\uparrow$)} & 
%\multirow{2}{*}{\textbf{PTM} ($\uparrow$)} 
& & \multirow{2}{*}{\textbf{Inverse Folding} ($\downarrow$)}  \\
\cmidrule{6-8}
 & \multicolumn{1}{c}{} &  & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & Fold & Family & Superfamily &  & \\

% \midrule 

%\midrule 
%\multirow{2}{*}{SchNet} & \caa + \virt &  &
%58.68 / -------- / 57.83  
%&  & 18.01 / 18.16 / \colorbox{purple!20}{\underline{25.94}}  & 73.65 / 74.43 / \colorbox{purple!20}{\underline{88.76}} & 23.17 / 21.09 / \colorbox{purple!20}{\underline{38.41}} &  &  & 9.71  \\

 %& \caa + \virt + \bb &  & 
 %56.54 / -------- / --------  
 %&  &  16.63 / \colorbox{green!20}{18.25} / 17.75  & 69.5 / 70.40 /  \colorbox{purple!20}{73.22}  & 21.17 / \colorbox{green!20}{23.50} / 23.39 & & & 9.44  \\
%\multicolumn{1}{l}{\multirow{2}{*}{DimeNet}} & \caa + \virt &  &  &  & -------- / -------- / -------- & -------- / -------- / --------& -------- / -------- /--------  & & -------- / -------- / --------&  --------  &  &  &  \\

%\multicolumn{1}{l}{} & \caa + \virt + \bb &  &  & & -------- / -------- / -------- & -------- / -------- / --------& -------- / -------- /-------- &  & -------- / -------- / -------- & -------- &  &  &  &  \\

\midrule
\multicolumn{1}{l}{\multirow{2}{*}{GearNet}} & \caa + \virt &  &  &  & -------- / 36.00 / \colorbox{green!20}{40.54}  & -------- / 94.00 / \colorbox{green!20}{\underline{97.68}} & -------- /  49.58 / \colorbox{green!20}{\underline{55.60}} &  & & 12.35 / \colorbox{purple!20}{7.84} \\

\multicolumn{1}{l}{} & \caa + \virt + \bb &  &  &  & 39.92 / 38.45 / \colorbox{green!20}{\underline{42.94}}  & \colorbox{orange!20}{96.26} / 96.15 / 96.17 & 53.00 / 51.79 / \colorbox{green!20}{54.51} & &  & 11.61 / \colorbox{purple!20}{\underline{7.29}} \\

%\midrule
%\multicolumn{1}{l}{\multirow{2}{*}{EGNN}} & \caa + \virt &  &  &  &  \underline{26.53} / -------- / -------- & \underline{93.19} / -------- / --------  & \underline{33.76} / -------- / --------&  & \colorbox{blue!20}{\underline{96.65}} / 96.29 / 96.29 & 8.71  \\

%\multicolumn{1}{l}{} & \caa + \virt + \bb &  &  & & 21.75 / -------- / -------- & 86.76 / -------- / --------& 30.04 / -------- / -------- &  & 96.02 / \colorbox{green!20}{96.08} / 95.71 & 9.59 & &  &  &  \\

\midrule
\multicolumn{1}{l}{\multirow{2}{*}{GCPNet}} & \caa + \virt &  &  &  & 43.92 / 44.76 / \colorbox{green!20}{48.37} & 97.93 / 98.41 / \colorbox{green!20}{\textbf{\underline{98.27}}} & 56.28 / 60.47 / \colorbox{green!20}{62.64} &  &  & 8.80 / \colorbox{purple!20}{7.37}  \\

\multicolumn{1}{l}{} & \caa + \virt + \bb  &  &  &  &  44.76 / 47.53 / \colorbox{green!20}{\textbf{\underline{49.53}}} & 97.55 / 97.71 / \colorbox{green!20}{97.98} & 56.83 / 59.31 / \colorbox{green!20}{\textbf{\underline{64.54}}} & & & 7.56 / \colorbox{purple!20}{\textbf{\underline{6.55}}} \\

\bottomrule
\end{tabular}
\end{adjustbox}
\end{table}

% This suggests that, compared to backbone dihedral angles, virtual angles may provide useful information that is more easily transferable between AlphaFold-predicted structures and experimental protein structures in the context of pretraining. However, we note that pretrained results with backbone dihedral angles still yield improved results over non-pretrained baselines. Also interestingly, from the results in Table \ref{tab:pre_trained_graph_classification_results} it appears that (2) structure denoising serves equivariant models such as GCPNet best for performance in downstream tasks.
